/**
 * ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ í•¸ë“¤ëŸ¬
 * 3ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì „ì²´ì— ëŒ€í•œ ë°°ê²½ íŒŒë¼ë¯¸í„° ìƒì„±
 * 
 * í”„ë¡œì„¸ìŠ¤:
 * 1. ë¡œê·¸ì¸ ì„¸ì…˜ ê¸°ì¤€ìœ¼ë¡œ ê°ì • ì¹´ìš´í„° ì¡°íšŒ ë° í´ë Œì§•
 * 2. Python ì„œë²„ì—ì„œ ê°ì • ì˜ˆì¸¡ ë°›ê¸° (ì „ì²˜ë¦¬ ë°ì´í„° + ê°ì • ì¹´ìš´í„°)
 * 3. Python ì‘ë‹µ ê²€ì¦
 * 4. Python ì‘ë‹µ JSONì„ ê·¸ëŒ€ë¡œ LLM í”„ë¡¬í”„íŠ¸ì— í¬í•¨
 * 5. LLMìœ¼ë¡œ ë°°ê²½ íŒŒë¼ë¯¸í„° ìƒì„±
 */

import { NextResponse } from "next/server";
import { prepareLLMInput, type LLMInput } from "@/lib/llm/prepareLLMInput";
import { generatePromptFromPythonResponse } from "@/lib/llm/optimizePromptForPython";
import { validateAndNormalizeResponse, type BackgroundParamsResponse } from "@/lib/llm/validateResponse";
import { getCachedResponse, setCachedResponse } from "@/lib/cache/llmCache";
import { getMockResponse } from "../utils/mockResponse";
import { PythonEmotionPredictionProvider } from "@/lib/prediction/PythonEmotionPredictionProvider";
import { validatePythonResponse } from "@/lib/prediction/validatePythonResponse";
import type { PythonPredictionResponse } from "@/lib/prediction/types";
import { getAndResetEmotionCounts } from "@/lib/emotionCounts/EmotionCountStore";
import type { EmotionPredictionInput } from "@/lib/prediction/EmotionPredictionProvider";
import type { StreamHandlerParams } from "../types";
import OpenAI from "openai";

export async function handleStreamMode({
  segments,
  preprocessed,
  moodStream,
  userPreferences,
  forceFresh,
  userId, // ë¡œê·¸ì¸ ì„¸ì…˜ ê¸°ì¤€
  session, // ëª©ì—… ëª¨ë“œ í™•ì¸ìš©
}: StreamHandlerParams): Promise<NextResponse> {
  if (!segments || !Array.isArray(segments) || segments.length === 0) {
    // segmentsê°€ ì—†ìœ¼ë©´ ëª©ì—… ì‘ë‹µ ë°˜í™˜
    return NextResponse.json(getMockResponse());
  }

  // 3ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¡œ LLM Input ì¤€ë¹„ (ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ëŒ€í‘œë¡œ ì‚¬ìš©)
  const firstSegment = segments[0];

  // emotionEventsê°€ undefinedì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì œê³µ
  const preprocessedWithDefaults = {
    ...preprocessed,
    emotionEvents: preprocessed.emotionEvents || {
      laughter: [],
      sigh: [],
      anger: [],
      sadness: [],
      neutral: [],
    },
  };

  const llmInput = await prepareLLMInput(
    preprocessedWithDefaults,
    {
      currentMood: {
        id: firstSegment.mood?.id || "",
        name: firstSegment.mood?.name || "",
        cluster: "0",
        music: {
          genre: firstSegment.mood?.music?.genre || "",
          title: firstSegment.mood?.music?.title || "",
        },
        scent: {
          type: firstSegment.mood?.scent?.type || "",
          name: firstSegment.mood?.scent?.name || "",
        },
        lighting: {
          color: firstSegment.mood?.lighting?.color || "#E6F3FF",
          rgb: firstSegment.mood?.lighting?.rgb || [230, 243, 255],
        },
      },
      userDataCount: moodStream.userDataCount || 0,
    },
    userPreferences
  );

  // ===== 1. ë¡œê·¸ì¸ ì„¸ì…˜ ê¸°ì¤€ìœ¼ë¡œ ê°ì • ì¹´ìš´í„° ì¡°íšŒ ë° í´ë Œì§• =====
  const emotionCounts = getAndResetEmotionCounts(userId);
  console.log(`[Stream Handler] Emotion counts for user ${userId}:`, {
    laughter: emotionCounts.laughter,
    sigh: emotionCounts.sigh,
    crying: emotionCounts.crying,
    accumulationDuration: Math.floor((Date.now() - emotionCounts.lastResetTime) / 1000),
  });

  // ì „ì²˜ë¦¬ ë°ì´í„°ì— ê°ì • ì¹´ìš´í„° ì¶”ê°€
  const preprocessedWithCounts = {
    ...preprocessed,
    emotionCounts: {
      laughter: emotionCounts.laughter,
      sigh: emotionCounts.sigh,
      crying: emotionCounts.crying,
    },
    accumulationDurationSeconds: Math.floor((Date.now() - emotionCounts.lastResetTime) / 1000),
    lastResetTime: emotionCounts.lastResetTime,
  };

  let pythonResponse: PythonPredictionResponse | null = null;

  try {
    // ===== 2. Python ì„œë²„ì—ì„œ ê°ì • ì˜ˆì¸¡ ë°›ê¸° =====
    // PYTHON_SERVER_URL ì´ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” Python ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³ 
    // ë°”ë¡œ LLM-only fallback ìœ¼ë¡œ ì§„í–‰í•œë‹¤.
    if (!process.env.PYTHON_SERVER_URL) {
      console.warn(
        "[Stream Handler] PYTHON_SERVER_URL not set. Skipping Python step and using LLM-only fallback."
      );
      pythonResponse = null;
    } else {
      const pythonProvider = new PythonEmotionPredictionProvider();

      const predictionInput: EmotionPredictionInput = {
        preprocessed: preprocessedWithCounts,
        currentTime: Date.now(),
        segmentCount: 3,
      };

      // Python ì„œë²„ í˜¸ì¶œ
      pythonResponse = await pythonProvider.getPythonResponse(predictionInput);

      // Python ì‘ë‹µ ê²€ì¦
      if (!validatePythonResponse(pythonResponse)) {
        console.error("[Stream Handler] Invalid Python response, using fallback");
        pythonResponse = null;
      } else {
        console.log("\n" + "=".repeat(80));
        console.log("âœ… [Stream Handler] Python response validated successfully:");
        console.log("=".repeat(80));
        console.log(JSON.stringify(pythonResponse, null, 2));
        console.log("=".repeat(80) + "\n");
      }
    }
  } catch (pythonError) {
    console.error("[Stream Handler] Python server error, using fallback:", pythonError);
    pythonResponse = null;
  }

  // Python ì‘ë‹µì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
  if (!pythonResponse) {
    console.warn("[Stream Handler] Python response not available, falling back to original prompt");
  return handleStreamModeFallback({
    segments,
    preprocessed,
    moodStream,
    userPreferences,
    forceFresh,
    llmInput,
    userId,
    session,
  });
  }

  // ===== 3. Python ì‘ë‹µ JSONì„ ê·¸ëŒ€ë¡œ LLM í”„ë¡¬í”„íŠ¸ì— í¬í•¨ =====
  // ì„¸ì…˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ëª©ì—… ëª¨ë“œ í™•ì¸
  const prompt = await generatePromptFromPythonResponse(llmInput, pythonResponse, userId, segments, session);

  // ìºì‹œ í™•ì¸ (Python ì‘ë‹µ í¬í•¨)
  const cacheKey = {
    moodName: llmInput.moodName,
    musicGenre: llmInput.musicGenre,
    scentType: llmInput.scentType,
    timeOfDay: llmInput.timeOfDay || new Date().getHours(),
    season: llmInput.season || "Winter",
    stressIndex: preprocessed.recent_stress_index,
    segmentIndex: undefined,
    pythonCurrentId: pythonResponse.current_id,
    pythonFutureId: pythonResponse.future_id,
  };

  if (!forceFresh) {
    const cachedResponse = getCachedResponse(cacheKey);
    if (cachedResponse) {
      console.log("[LLM Cache] Cache hit (with Python response), returning cached response");
      return NextResponse.json({ ...cachedResponse, source: "cache" });
    }
  }

  // ===== 4. LLMìœ¼ë¡œ ë°°ê²½ íŒŒë¼ë¯¸í„° ìƒì„± =====
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    console.warn("OPENAI_API_KEY not found, using mock response");
    const mockResponse = { ...getMockResponse(), source: "mock-no-key" as const };
    setCachedResponse(cacheKey, mockResponse);
    return NextResponse.json(mockResponse);
  }

  const openai = new OpenAI({ apiKey });

  try {
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: "JSONë§Œ ì‘ë‹µ" },
        { role: "user", content: prompt },
      ],
      response_format: { type: "json_object" },
      temperature: 0.9,
      max_tokens: 3000,
    });

    const rawResponse = JSON.parse(completion.choices[0].message.content || "{}");
    
    // ===== LLM ì›ì‹œ ì‘ë‹µ ë¡œê¹… =====
    console.log("\n" + "=".repeat(80));
    console.log("ğŸ¨ [LLM API] Raw Response from OpenAI (with Python input):");
    console.log("=".repeat(80));
    console.log(JSON.stringify(rawResponse, null, 2));
    console.log("=".repeat(80) + "\n");
    
    const validatedResponse = validateAndNormalizeResponse(rawResponse);
    
    // ===== ê²€ì¦ëœ ì‘ë‹µ ë¡œê¹… =====
    console.log("\n" + "=".repeat(80));
    console.log("âœ… [LLM API] Validated Response (with Python input):");
    console.log("=".repeat(80));
    if ('segments' in validatedResponse && Array.isArray(validatedResponse.segments)) {
      console.log(`Total segments: ${validatedResponse.segments.length}`);
      validatedResponse.segments.forEach((seg, idx) => {
        console.log(`\n--- Segment ${idx} ---`);
        console.log(`  moodAlias: "${seg.moodAlias}"`);
        console.log(`  musicSelection: "${seg.musicSelection}"`);
        console.log(`  moodColor: "${seg.moodColor}"`);
        console.log(`  backgroundIcon: { name: "${seg.backgroundIcon.name}", category: "${seg.backgroundIcon.category}" }`);
      });
    }
    console.log("=".repeat(80) + "\n");
    
    // ìºì‹œ ì €ì¥
    const cacheResponse: BackgroundParamsResponse = 'segments' in validatedResponse && Array.isArray(validatedResponse.segments)
      ? validatedResponse.segments[0]
      : validatedResponse as BackgroundParamsResponse;
    setCachedResponse(cacheKey, cacheResponse);
    
    return NextResponse.json({ 
      ...validatedResponse, 
      source: "openai",
      pythonResponse: pythonResponse, // Python ì‘ë‹µë„ í•¨ê»˜ ë°˜í™˜ (ë””ë²„ê¹…/ê²€ì¦ìš©)
    });
  } catch (openaiError) {
    console.error("[LLM API] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨:", openaiError);
    const mockResponse = { ...getMockResponse(), source: "mock-openai-error" as const };
    setCachedResponse(cacheKey, mockResponse);
    return NextResponse.json(mockResponse);
  }
}

/**
 * Fallback í•¸ë“¤ëŸ¬ (Python ì‘ë‹µ ì—†ì„ ë•Œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)
 */
async function handleStreamModeFallback({
  segments,
  preprocessed,
  moodStream,
  userPreferences,
  forceFresh,
  llmInput,
  userId,
  session,
}: Pick<StreamHandlerParams, "segments" | "preprocessed" | "moodStream" | "userPreferences" | "forceFresh" | "userId"> & {
  llmInput: LLMInput;
  session?: { user?: { email?: string; id?: string } } | null;
}): Promise<NextResponse> {
  // ê¸°ì¡´ generateOptimizedPrompt ì‚¬ìš© (Python ì—†ì´)
  const { generateOptimizedPrompt } = await import("@/lib/llm/optimizePrompt");
  const prompt = generateOptimizedPrompt(llmInput, segments);
  
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    return NextResponse.json(getMockResponse());
  }

  const openai = new OpenAI({ apiKey });

  try {
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: "JSONë§Œ ì‘ë‹µ" },
        { role: "user", content: prompt },
      ],
      response_format: { type: "json_object" },
      temperature: 0.9,
      max_tokens: 3000,
    });

    const rawResponse = JSON.parse(completion.choices[0].message.content || "{}");
    const validatedResponse = validateAndNormalizeResponse(rawResponse);
    
    return NextResponse.json({ ...validatedResponse, source: "openai-fallback" });
  } catch (error) {
    console.error("[Stream Handler Fallback] Error:", error);
    return NextResponse.json(getMockResponse());
  }
}
