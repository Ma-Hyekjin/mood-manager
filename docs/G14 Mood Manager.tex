\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage[hidelinks]{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Mood Manager\\
{\large AI-Based Personalized Mood Management System for Smart Homes}}

\author{
\IEEEauthorblockN{Hyeokjin Ma}
\IEEEauthorblockA{\textit{Dept. of Information Systems}\\
\textit{Hanyang University}\\
Seoul, Republic of Korea\\
tema0311@hanyang.ac.kr}
\and
\IEEEauthorblockN{Hyunwoo Choi}
\IEEEauthorblockA{\textit{Dept. of Information Systems}\\
\textit{Hanyang University}\\
Seoul, Republic of Korea\\
hhyyrr0713@hanyang.ac.kr}
\and
\IEEEauthorblockN{Junseong Ahn}
\IEEEauthorblockA{\textit{Dept. of Information Systems}\\
\textit{Hanyang University}\\
Seoul, Republic of Korea\\
lljs1113@hanyang.ac.kr}
\and
\IEEEauthorblockN{Saeyeon Park}
\IEEEauthorblockA{\textit{Dept. of Information Systems}\\
\textit{Hanyang University}\\
Seoul, Republic of Korea\\
saeyeon0317@hanyang.ac.kr}
\and
\IEEEauthorblockN{Heejoo Chae}
\IEEEauthorblockA{\textit{Dept. of Information Systems}\\
\textit{Hanyang University}\\
Seoul, Republic of Korea\\
heeju0203@hanyang.ac.kr}
}

\maketitle

\begin{abstract}
Smart homes today offer convenience and automation, yet their fragmented control of lighting, fragrance, and sound leaves the overall spatial experience disjointed and reactive. Existing systems focus on functional operation but lack the ability to anticipate user needs or create holistic, personalized environments. To address this gap, we present Mood Manager, an AI-driven virtual device that integrates ambient elements—light, scent, and audio—into a unified, context-aware system. Using few-shot prompting as its personalization engine, Mood Manager learns user preferences dynamically through real-time interactions, requiring no dedicated model retraining. By combining contextual awareness, feedback logging, and IoT-based prototyping, the system not only responds to explicit commands but also proactively curates atmospheres tailored to user states and environmental conditions. Over time, Mood Manager evolves into an autonomous "mood curator," transforming ordinary spaces into hyper-personalized therapeutic environments. This approach demonstrates how hyper-personalization, powered by lightweight AI techniques, can redefine smart home experiences from passive control toward active well-being enhancement.
\end{abstract}

\section{Role Assignments}

\begin{table*}[ht]
\centering
\caption{Role Assignments}
\begin{tabular}{|p{2.5cm}|p{2cm}|p{11cm}|}
\hline
\textbf{Roles} & \textbf{Name} & \textbf{Task description and etc.} \\
\hline
Software Developer (Full-stack) & Hyeokjin Ma & Manages the end-to-end integration of the system across the web client, backend processes, and data synchronization layers. Designs the flow between WearOS inputs, Firebase streams, preprocessing logic, and AI inference endpoints to maintain stability and coherence. Oversees the rollout of core features, identifies bottlenecks, and ensures reliable communication across modules. Coordinates development schedules, enforces shared data standards, and maintains documentation for architecture decisions. Produces integration diagrams, technical reports, and performance evaluations used throughout development. \\
\hline
Software Developer (Front-end) & Hyunwoo Choi & Designs and implements the user-facing interface with emphasis on clarity, responsiveness, and a consistent visual system. Builds interactive layouts for mood control, device management, and user onboarding while ensuring smooth navigation across the application. Refines UI through testing, accessibility checks, and iterative design improvements. Collaborates with backend and AI developers to reflect real-time biometric and event-driven data in the dashboard. Produces component libraries, design guidelines, and interaction documentation that support long-term scalability. \\
\hline
Software Developer (Back-end) & Heejoo Chae & Leads the development of the server-side architecture responsible for authentication, data processing, and API scalability. Designs the Prisma database schema, implements route logic for device operations and mood updates, and maintains secure communication between the web client, Firebase, and preprocessing workflows. Manages integration of datasets to support personalized recommendations. Coordinates closely with AI developers to ensure consistent data formatting and collaborates with the full-stack developer to maintain API coherence. Produces backend specifications, schema revisions, operational manuals, and monitoring procedures for system reliability. \\
\hline
AI Developer & Saeyeon Park & Works on building the LLM-based inference pipeline using structured few-shot prompting and adaptive reasoning techniques. Designs prompt templates that convert biometric signals, preference data, and contextual indicators into unified mood profiles. Tunes generation parameters to improve relevance, reduce variance, and maintain narrative consistency across moods. Cooperates with the multimodal AI developer to align semantic cues from physiological and audio data, and with backend engineers to validate JSON schema compatibility. Produces experiment summaries, iteration logs, and optimization guidelines that refine the mood-inference engine over time. \\
\hline
AI Developer & Junseong Ahn & Focuses on integrating physiological, acoustic, and environmental signals from WearOS and external APIs into a coherent representation of user state. Develops preprocessing routines for HRV, stress indices, sleep metrics, and audio-event classification to support personalized mood prediction. Works closely with backend developers to stabilize the data ingestion pipeline and ensure the real-time delivery of multimodal features. Collaborates with the LLM-focused AI developer to translate raw signals into interpretable inputs for the recommendation engine. Contributes to improving system robustness, context awareness, and privacy-preserving data handling throughout the full processing pipeline. \\
\hline
\end{tabular}
\end{table*}

\section{Introduction}

\subsection{Motivation}

\subsubsection{The Rise of Hyper-Personalization in AI}

Artificial intelligence has transitioned from basic automation to highly adaptive and user-centric systems. Early applications of AI mainly focused on efficiency and scalability, offering generalized solutions designed for broad user groups. However, with the introduction of large language models (LLMs) and generative AI, the paradigm has shifted toward experiences that dynamically adapt to individuals. Users now expect technology not only to execute tasks but also to understand personal preferences, anticipate needs, and respond in intuitive, human-like ways. This shift marks the emergence of hyper-personalization — the ability to tailor services to the unique emotional and behavioral patterns of each user.

Among recent personalization techniques, few-shot prompting has become one of the most effective approaches for achieving real-time adaptation. Unlike traditional retraining or fine-tuning methods, few-shot prompting enables AI systems to learn user tendencies instantly through a small number of examples. This allows the model to refine its behavior continuously based on individual feedback without requiring extensive computation or model updates. As a result, AI evolves into a responsive companion capable of forming emotional resonance with users, delivering experiences that feel deeply personal and contextually aware — extending far beyond conventional automation.

\subsubsection{The Current Gap in Smart Home Environments}

\paragraph{Current State of Smart Home Systems}
The global smart home market has expanded rapidly, offering consumers a wide range of connected devices that support automation, remote control, and energy management. From smart speakers and lighting systems to intelligent thermostats, these technologies have enhanced convenience and accessibility in daily life. However, most devices still operate independently, relying on user-initiated commands rather than seamless coordination between systems. Research indicates that while the adoption rate of smart devices is high, advanced features such as cross-device automation and contextual adaptation remain underutilized. More than half of users engage only with basic functions such as turning devices on or off, leaving advanced integrations unexplored. This reflects a fundamental gap between technological potential and the actual user experience—where automation exists, but intelligent interaction and emotional resonance do not.

\paragraph{Current Trends in the Home Fragrance Market}
In parallel, the home fragrance market has shown remarkable growth, driven by consumer interest in wellness, self-care, and emotional comfort within living spaces. Scent has become an essential factor in shaping atmosphere and mood, complementing lighting and sound to create immersive environments. Recent developments include smart diffusers and app-controlled scent devices; however, these solutions largely remain standalone products with limited integration into broader smart home ecosystems. While lighting and audio technologies have achieved sophisticated automation and voice control, fragrance devices are still predominantly manual and lack context-aware functionality. This imbalance reveals an untapped opportunity—bridging the gap between emotional personalization and technological integration through intelligent, automated scent management.

\paragraph{Need for Integrated Multi-Sensory Control}
To achieve a truly intelligent home environment, it is essential to technologically integrate lighting, sound, and fragrance into a single adaptive framework. Each sensory component influences human emotion in complementary ways: lighting affects focus and relaxation, sound shapes perception and mood, and fragrance modulates emotional comfort. By connecting these elements through AI-driven personalization and IoT synchronization, users can experience holistic, context-aware mood management rather than fragmented device control. This integration establishes the foundation for the proposed Mood Manager System, which coordinates sensory feedback and user interaction to deliver adaptive, emotionally intelligent smart home experiences.

\subsubsection{Intelligent Mood Management System}

Our project introduces the concept of the Mood Manager, a system designed to harmonize lighting, sound, and fragrance into a unified, intelligent experience. Unlike conventional smart home systems that require explicit commands, Mood Manager uses contextual cues and user feedback to infer preferences and deliver proactive adjustments. By employing few-shot prompting as its personalization engine, the system dynamically learns from minimal interaction data, adapting to individual users without the need for extensive retraining. This capability allows Mood Manager to evolve with each interaction, gradually curating an environment that feels increasingly attuned to the user's lifestyle and emotional needs.

The vision is to transform personal spaces into adaptive, therapeutic environments—"personalized therapy rooms" that respond seamlessly to mood and context. For example, on a rainy day, Mood Manager recalls the user's preferred warm lighting, calming fragrance, and soft background music, recreating a comforting atmosphere without requiring manual intervention. Ultimately, the project seeks to demonstrate how hyper-personalized AI, combined with IoT integration, can redefine smart homes as environments that enhance not only convenience but also emotional well-being and quality of life.

\subsection{Problem Statement (Client's needs)}

\subsubsection{Fragmented Customization and Limited User Experience}

Although smart home devices advertise convenience and personalization, the actual user experience remains fragmented. Lighting, fragrance, and sound—three core factors that shape the ambiance of a personal space—are often managed separately through individual applications or manual controls. According to market surveys, one of the top dissatisfaction factors among smart home users is the lack of seamless interoperability across devices. Users may configure settings individually but fail to fully leverage the advanced functions designed for cross-device scenarios. This leads to declining satisfaction despite increased device ownership. There is a clear need for improved customization and a differentiated user experience that can integrate these elements into a unified and meaningful interaction.

\subsubsection{Static and Uniform Scenarios}

Routine-based automation is a common feature in smart home systems, allowing users to schedule repetitive tasks such as turning on lights or playing background music at fixed times. While useful for simple patterns, such routines fall short in dynamic and unpredictable situations. For example, sudden changes in weather, irregular user moods, or unexpected guest visits are scenarios where rigid routines fail to adapt. Furthermore, current automation functions cannot flexibly manage complex, multi-sensory interactions across lighting, fragrance diffusion, and soundscapes. This limitation reveals the need for systems capable of real-time contextual awareness, leveraging AI and machine learning to adapt to diverse and complex living scenarios.

\subsubsection{Overreliance on Passive and Manual Management}

Despite their "smart" label, most home devices still require significant user intervention for configuration and operation. Users must decide how each device should behave in specific contexts, from adjusting an air conditioner's temperature to selecting fragrance intensity or curating playlists. Even when routines are available, users must manually design logical structures, which can lead to errors or inefficiencies. This overreliance on manual control burdens users with unnecessary complexity, keeping the focus on independent device operation rather than holistic optimization of the entire home environment.

\subsubsection{Lack of Integrated Information and Orchestration}

Smart devices individually generate rich data, such as energy consumption, device usage patterns, or even wellness-related metrics. However, this information is siloed within separate applications, requiring users to manually gather, analyze, and act on disparate data sources. For example, checking air quality data, lighting conditions, and ambient sound levels often involves switching between multiple apps without any overarching integration. The lack of information synthesis prevents users from receiving proactive and contextually relevant recommendations. Without an intelligent system to integrate and orchestrate these insights, the promise of a truly seamless smart home experience remains unfulfilled.

\subsection{Research on Related Software}

\subsubsection{Home Assistant}
Home Assistant is an open-source smart-home automation platform that allows users to connect and manage devices from multiple manufacturers in one interface. It supports automation through YAML scripts and plug-ins, enabling complex routines and data visualization dashboards. However, its automation remains rule-based rather than AI-adaptive, and it lacks mood-oriented personalization that learns user emotions or context dynamically.

\subsubsection{openHAB (Open Home Automation Bus)}
openHAB is a Java-based, vendor-neutral smart-home integration framework. It connects heterogeneous IoT devices through a modular "binding" system and provides local dashboards for user-defined automation rules. While it is powerful for interoperability, openHAB depends on manually defined logic and does not include AI-driven personalization or cross-sensory control such as lighting–sound–fragrance orchestration.

\subsubsection{Philips Hue App + API}
Philips Hue provides advanced smart-lighting control software that allows users to adjust brightness, color temperature, and dynamic scenes through a mobile app or API. It integrates with third-party platforms such as Spotify for light-music synchronization, creating partial multi-sensory experiences. Despite its mature lighting automation, Hue's software ecosystem lacks integration with fragrance or contextual AI personalization, limiting full environmental adaptation.

\subsubsection{Pura Smart Fragrance App}
Pura's mobile software manages Wi-Fi-connected fragrance diffusers, letting users schedule scents, monitor cartridge levels, and adjust intensity remotely. The app also connects Alexa and Google Home for voice-based control. Although Pura digitalizes scent management effectively, it operates as an isolated fragrance-control application and cannot coordinate scent with lighting or sound in an AI-driven manner.

\subsubsection{Amazon Alexa Routines}
Alexa Routines software enables users to automate tasks such as turning on lights or playing music based on time, sensor data, or voice triggers. It can execute multiple actions under a predefined "mood" routine (e.g., "Good Morning" or "Relax"). However, its logic is preprogrammed rather than contextual; it does not automatically adapt to environmental changes such as weather or user emotional state.

\subsubsection{Google Home / Nest App}
Google Home application serves as the control hub for devices powered by Google Assistant. It integrates voice commands, visual feedback, and multi-room media control, learning user habits to improve automation suggestions. While it demonstrates conversational intelligence, it mainly supports command execution and lacks true hyper-personalization or multi-sensory mood synthesis.

\subsubsection{Recombee AI Recommendation Engine}
Recombee is a cloud-based recommendation-engine software that uses collaborative filtering and neural models to predict user preferences in real time. In the context of smart environments, its recommendation logic could inspire how the Mood Manager suggests optimal combinations of lighting, sound, and fragrance. Nevertheless, Recombee is oriented toward digital content recommendation rather than physical-environment orchestration.

\subsubsection{Dynamic Yield Personalization Platform}
Dynamic Yield is an AI-driven personalization software that analyzes behavioral data to adapt content and optimize user experiences across digital interfaces. Its continuous learning framework and A/B testing tools illustrate how adaptive systems can refine personalization through feedback loops. Although designed for marketing applications, its approach parallels the Mood Manager's objective of refining environmental recommendations based on user responses and context data.

\section{Requirements}

\subsection{Sign up}

Mood Manager requires essential personal and preference information to create a personalized smart-home profile. The system collects user credentials and baseline data to enable AI-driven customization from the first use.

\subsubsection{Enter Email Address}
Users must provide a valid email address as their primary account identifier. The email is verified for format validity and checked for duplicates during registration.

\subsubsection{Create Password}
Users are required to create a password with at least six characters. As users input their password, a real-time indicator displays its strength (Weak / Medium / Strong) to enhance security awareness.

\subsubsection{Enter Name}
The user's name is collected and automatically set as the default nickname for their account. This name is also used by the AI for conversational interactions and natural responses.

\subsubsection{Select Gender}
Gender selection is optional and used solely to improve fragrance and sound personalization (e.g., preferred scent tone, music genre). Users may skip this step if they prefer not to disclose the information.

\subsubsection{Enter Birth Date}
Users enter their full birth date (YYYY-MM-DD format). The system validates that users are at least 12 years old, not the full date of birth, to protect privacy while enabling broad personalization (e.g., age-based recommendation tendencies).

\subsubsection{Agree to Data Usage \& Privacy Policy}
Users must read and consent to the data usage policy, explaining how their environmental preferences and feedback logs are stored and used to improve AI personalization.

\subsection{Sign In}

Mood Manager provides flexible login options to ensure convenience and security for users.

\subsubsection{Local Login through Mood Manager Account}
Users can log in by entering their registered email and password. If authentication is successful, a secure session token (JWT) is issued to maintain access throughout the session. In case of failure, a pop-up message such as "Invalid ID or password" is displayed with an option to reset the password.

\subsubsection{Social Login via SNS Integration}
Mood Manager supports quick login through external services such as Google, KakaoTalk (Kakao), or Naver. When users select a social login option, the system authenticates via the provider's OAuth 2.0 API and retrieves essential profile data (name, email, profile image). This data is automatically linked to the user's Mood Manager account.

\subsubsection{Auto-Login and Session Management}
For convenience, the system offers an "Auto Login" toggle that securely stores encrypted session tokens on the device. When enabled, the user can access the system directly without repeated authentication, provided the session remains valid.

\subsection{Device Registration}

After signing in, users can register their smart devices and initialize personalization features in Mood Manager. Device registration serves as the foundation for AI-driven mood control, linking physical appliances with user preferences and environmental data. There are two stages: (1) registering connected IoT devices and (2) configuring personal and environmental settings for mood adaptation.

\subsubsection{Device Registration}
Users can connect their smart devices to Mood Manager manually. Users can register devices by scanning the QR code printed on the product. Upon granting permission, the system automatically identifies the product model and connects it to the user's account. Alternatively, users may search by product name, serial number, or wireless protocol (Wi-Fi/Bluetooth).

\subsubsection{Preference Survey}
After device registration, users complete a short preference survey to initialize the AI's personalization engine. This survey helps the system understand basic mood-related tendencies and sensory preferences.

\paragraph{Lighting Preferences}
Users select their favorite light colors from preset options (warmWhite, skyBlue, softPink, etc.) and indicate any disliked colors. Multiple selections are stored as comma-separated strings in UserPreferences.colorLiked and colorDisliked fields (e.g., warm, cool, natural). The AI later uses this data to suggest appropriate lighting scenes based on time, weather, or activity.

\paragraph{Fragrance Preferences}
Users choose from basic fragrance families such as floral, woody, citrus, or fresh. They can also indicate sensitivities or allergies (e.g., "avoid strong musk scents").

\paragraph{Sound Preferences}
Users specify their preferred audio genres or soundscapes (e.g., lo-fi, jazz, ambient nature sounds) and desired volume levels during specific moods (e.g., focus, relaxation).

\paragraph{Mood Mapping}
Users can connect moods (e.g., relaxed, focused, romantic) with their chosen lighting, fragrance, and sound combinations. These mappings act as initial training data for the AI's few-shot prompting model, allowing it to predict suitable combinations in new contexts.

\subsubsection{AI Initialization and Verification}
After device registration, preference input, and environmental setup are complete, Mood Manager's AI Personalization Engine is initialized. The system verifies all connected devices and settings through a brief diagnostic step, ensuring successful linkage and readiness for automatic mood control.

\subsection{Mood Selection}

The Mood Selection feature enables users to experience AI-driven, hyper-personalized environments through the integrated control of lighting, fragrance, and sound. Based on the user's registered devices, preferences, and contextual data, the system automatically recommends or adjusts the optimal "mood scene." Users can also manually modify each element through intuitive interfaces within the application.

\subsubsection{Mood Recommendation and Scene Generation}

\paragraph{AI-Powered Context-Aware Analysis}
The system continuously monitors environmental variables such as biometric data from WearOS devices (heart rate, HRV, stress levels), audio events from Firebase Firestore (laughter/sigh detection), and user preferences. This contextual information is processed by the AI Personalization Engine, which uses a Few-shot Prompting mechanism to infer the most appropriate combination of light, sound, and fragrance for the current situation.

\paragraph{Mood Scene Suggestion}
The AI generates one or more mood scene options (e.g., Relaxation, Focus, Romantic Evening) in JSON format, containing recommended values for brightness, fragrance intensity, and music volume. Users can preview and select a preferred mood scene or allow the system to apply it automatically.

\paragraph{Manual Adjustment and Override}
Users can adjust each element of the mood scene individually (e.g., slightly dimmer lighting, lower fragrance intensity). All adjustments are recorded as user feedback to refine future recommendations.

\subsubsection{Lighting Control}

\paragraph{Integration with Smart Lighting Devices}
Lighting control is implemented through protocols such as Matter or MQTT, enabling direct communication with smart bulbs or lighting panels (e.g., Philips Hue, Nanoleaf).

\paragraph{Brightness and Color Temperature Adjustment}
Based on the selected mood, the system modifies brightness and color tone—for example:
\begin{itemize}
\item Relax Mode: 30–40\% brightness, 2800–3000K warm tone
\item Focus Mode: 70–80\% brightness, 5000–6000K cool tone
\end{itemize}

\paragraph{Dynamic Lighting Effects}
For certain moods, dynamic transitions (fade-in/fade-out or rhythmic lighting) are used to enhance immersion. These effects are generated in real time through the lighting control API.

\subsubsection{Fragrance Control}

\paragraph{Smart Diffuser Integration}
The system communicates with Wi-Fi or BLE-enabled fragrance diffusers that support programmable intensity and duration.

\paragraph{Intensity and Duration Management}
The AI determines the appropriate scent intensity (Level 1–5) and dispersal duration based on user context and previous preferences. For example, during a Relax mood, a citrus or woody scent may be diffused at moderate intensity for 30 minutes.

\paragraph{Safety and Refill Monitoring}
The system includes safety guardrails such as preventing over-diffusion, enforcing cooldown intervals, and monitoring cartridge capacity. If refill levels are low, the user receives an in-app notification.

\subsubsection{Sound Control}

\paragraph{Music and Soundscape Selection}
Mood Manager connects to streaming APIs (e.g., Spotify, YouTube Music) or local storage to play background sounds aligned with the selected mood. Example pairings include:
\begin{itemize}
\item Focus: Lo-fi beats, white noise
\item Relax: Nature sounds, jazz
\item Romantic: Soft instrumental music
\end{itemize}

\paragraph{Volume and Playback Scheduling}
The system controls speaker volume through the IoT layer, maintaining balance with lighting and fragrance intensity. Playback automatically starts and stops according to mood duration or time of day.

\subsubsection{AI-driven Multi-Sensory Synchronization}
All three sensory components—light, sound, and fragrance—are orchestrated simultaneously by the IoT Orchestration Module. This module ensures that transitions occur smoothly and synchronously, creating a cohesive experience rather than separate device reactions. For instance, when the user switches from Focus to Relax, the lighting slowly dims while soft music and fragrance fade in together. The orchestration logic follows a transaction-based system that guarantees all connected devices receive the command successfully. If one device fails, the system automatically rolls back to the previous stable state to maintain harmony in the environment.

\subsection{Chat-Based Feedback}

The Chat-Based Feedback feature allows users to communicate naturally with the system after a mood scene has been applied. Through conversational feedback, users can express satisfaction, make adjustments, or request changes in real time. This interaction not only enhances user engagement but also serves as essential data for the AI Personalization Engine to refine future recommendations.

\subsubsection{Chat Interface for Feedback Collection}

\paragraph{Conversational Feedback Window}
After a mood scene (lighting, fragrance, and sound) is applied, the system opens a chat interface with the AI assistant. The assistant asks short, context-aware questions such as:
\begin{itemize}
\item "Do you like this current mood setup?"
\item "Would you prefer brighter lighting or a softer scent?"
\end{itemize}

\paragraph{User Responses}
Users can respond freely in natural language (e.g., "Make it a bit warmer" or "The scent is too strong"). The system's natural language processor (NLP) interprets the intent and modifies the settings in real time.

\paragraph{Quick Feedback Buttons}
For quick interaction, users can also select from predefined feedback buttons such as satisfied, unsatisfied, or adjust. These simple responses are recorded as sentimental data for learning purposes.

\subsubsection{Real-Time Adjustment and Data Logging}
When the user provides feedback, the system immediately modifies corresponding IoT parameters through the orchestration module. For instance, "Make it a little brighter" increases lighting brightness, while "Reduce the scent" decreases diffuser intensity. Each feedback session is stored in the feedback log, including contextual data (time, environment, and device states) and final configuration results. These logs are vectorized and stored in the embedding database, forming reference examples for Few-shot Prompting.

\subsubsection{AI Learning and Personalization Update}
The system periodically retrieves stored feedback data to enhance personalization. Positive or frequently accepted configurations are treated as "preferred examples," guiding future inference in similar contexts. The AI dynamically updates user profiles based on this data, gradually shaping individual tendencies such as preferred lighting tone or scent strength. When conflicting preferences occur, recency-based weighting ensures that the model adapts to the user's current lifestyle rather than outdated data.

\subsubsection{Emotion and Sentiment Recognition}
The NLP module analyzes the emotional tone of chat inputs to detect user sentiment (positive, neutral, or negative). If stress or fatigue-related language is detected (e.g., "I'm tired," "Too loud"), the system proactively adjusts to a Relax or Comfort mood scene. This sentiment-aware control enhances emotional intelligence and user satisfaction.

\section{Development Environment}

\subsection{Choice of software development platform}

\subsubsection{Development Platform}

\paragraph{Web Platform}
The web platform serves as the central foundation of the Mood Manager system, offering high accessibility, scalability, and compatibility across multiple operating systems. Through modern web technologies such as Next.js and TypeScript, developers can efficiently build responsive interfaces and maintain consistent user experiences on any device. The web platform supports seamless collaboration among team members using different operating systems including macOS, Windows, and Linux, as it only requires a web browser for development and testing. It also provides easy integration with cloud services such as Firebase Firestore for real-time data synchronization and AWS for hosting and deployment. Continuous updates in web technologies ensure compatibility with emerging standards and APIs, enabling long-term maintainability. For these reasons, the web platform is recognized as a flexible and future-proof environment that supports the core visualization and control components of the Mood Manager system.

\paragraph{Wear OS Platform}
Wear OS, built on the Android ecosystem, is employed as the data collection platform for biometric and acoustic information in the Mood Manager project. It provides robust SDK support and native integration with key APIs such as Health Connect, AudioRecord, and Porcupine, enabling efficient access to physiological data and on-device voice recognition features. Development in Kotlin enhances code safety, readability, and interoperability with Android libraries while ensuring optimal performance. The platform's ForegroundService feature allows continuous background operation for uninterrupted data capture, even when the watch display is inactive. Collected data are securely transmitted to Firebase Firestore for real-time synchronization with the cloud and web dashboard. As an Android-based system, Wear OS benefits from a large developer community, wide hardware availability, and strong compatibility with Google services. Due to these capabilities, Wear OS was selected as the optimal platform for reliable, real-time physiological data acquisition and contextual awareness within the Mood Manager ecosystem.

\subsubsection{Language}

\paragraph{Kotlin}
Kotlin is a modern, statically typed programming language officially supported by Google for Android and Wear OS development. It enhances code safety through null safety features and concise syntax, reducing boilerplate compared to Java. Kotlin provides seamless interoperability with existing Android libraries, enabling efficient use of APIs such as Health Connect and AudioRecord. Its coroutine-based concurrency allows smooth background processing and real-time data handling, making it ideal for continuous biometric and audio data collection in wearable environments.

\paragraph{TypeScript}
TypeScript is a superset of JavaScript developed by Microsoft, introducing a static type system that improves code stability and readability. It catches errors at compile time, reducing runtime issues and enhancing maintainability, especially in large-scale projects. TypeScript's strong type inference and modular structure help developers define clear code intent and foster collaboration across teams. It preserves JavaScript's flexibility while providing the safety of typed languages, making it a robust choice for building reliable and scalable web applications.

\paragraph{Python}
Python is a high-level, general-purpose programming language widely used for backend development, data analysis, and artificial intelligence applications. Its simple syntax and extensive library support make it suitable for rapid prototyping and deployment of complex systems. Python's ecosystem includes frameworks and tools optimized for web APIs, machine learning, and data processing, allowing seamless integration with cloud environments. Its flexibility and readability facilitate efficient backend logic implementation for data-driven decision-making in the Mood Manager system.

\subsubsection{Frameworks}

\paragraph{Next.js}
Next.js is a React-based web framework that provides server-side rendering and static site generation for fast and scalable web applications. It supports efficient routing and strong TypeScript integration, improving performance and maintainability. The framework simplifies both frontend and backend development through built-in API routes and automatic optimization, making it ideal for real-time interactive systems. It also provides excellent developer experience with hot reloading and simple deployment options, ensuring rapid iteration and stable production performance.

\paragraph{FastAPI}
FastAPI is a lightweight Python framework designed for building APIs quickly and efficiently. It uses Python type hints for automatic validation and documentation while supporting asynchronous processing for high-speed performance. The framework enables clean, readable, and easily extensible code, allowing developers to implement stable and responsive backend services. It is highly compatible with modern AI libraries and cloud environments, making it suitable for intelligent and data-driven applications.

\paragraph{Tailwind CSS}
Tailwind CSS is a utility-first framework that allows fast and consistent front-end design. It provides reusable style classes and encourages clean, responsive layouts without custom CSS. The framework ensures visual consistency, reduces repetitive styling tasks, and helps developers rapidly build modern and adaptive user interfaces. Tailwind also integrates seamlessly with component-based frameworks, enabling scalable and maintainable front-end design systems.

\paragraph{Android JetPack}
Android Jetpack is a collection of libraries and components that simplifies Android app development. It provides modules such as Lifecycle, ViewModel and WorkManager that help manage data, background tasks and UI states efficiently. Jetpack offers a standardized architecture, improves app stability, and enhances productivity with built-in tools for Wear OS integration. Its modular structure supports cleaner code organization and long-term maintenance for complex Android applications.

\subsubsection{Cloud Platform}

\paragraph{Amazon Web Services (AWS)}
AWS is a global cloud platform that provides hosting, storage and deployment services for modern applications. It offers high reliability, automatic scaling and strong security for both web and backend systems. AWS supports continuous integration and deployment pipelines, ensuring efficient updates and consistent performance across environments. Its broad service ecosystem allows flexible configuration and cost optimization, making it suitable for both development and large-scale production.

\paragraph{Firebase Firestore}
Firebase Firestore is a cloud-based NoSQL database that supports real-time synchronization and secure data storage. It allows fast data access and easy integration with mobile and web SDKs. Firestore automatically scales with usage and minimizes server management, enabling developers to focus on application functionality rather than infrastructure. Its real-time listener feature ensures that all connected clients instantly reflect changes, creating a seamless live data environment.

\subsubsection{Cost Estimation}

\paragraph{Hardware}
The hardware cost consists of laptops used by five team members for development and testing. Each device is estimated at approximately \textbackslash{}W1,500,000, which provides sufficient performance for running Android Studio, Next.js, and cloud-based tools. Since the team requires multi-platform compatibility across macOS and Windows environments, the total hardware expense amounts to \textbackslash{}W7,500,000 in total.

\paragraph{Software}
The software cost consists of cloud computing and AI model usage. For backend hosting and deployment, an AWS EC2 t3.xlarge instance is used, which provides 4 vCPUs, 16 GiB of memory, EBS-only storage, and network bandwidth up to 5 Gbps. The on-demand usage rate is USD 0.2816 per hour, approximately \textbackslash{}W370 per hour. Assuming a total usage of about 135 hours during the project development period, the total estimated cost for AWS is around \textbackslash{}W50,000. For AI inference, the project uses the OpenAI GPT-5 API, which is priced at USD 1.25 per one million input tokens and USD 10 per one million output tokens. The project is expected to use under one million input tokens and around three million output tokens, leading to an estimated total cost of USD 31.25, equivalent to approximately \textbackslash{}W41,000. Combining both components, the total estimated software cost is \textbackslash{}W91,000, covering cloud infrastructure and AI API usage throughout the development and testing phases.

\subsection{Software in Use}

\subsubsection{Android Studio}
Android Studio is the official integrated development environment for Android and Wear OS application development. It provides powerful tools for code editing, debugging, and performance profiling, allowing developers to build optimized and reliable mobile applications. The environment supports Kotlin and Java natively and integrates seamlessly with Android Jetpack, Health Connect, and other Google APIs. It also includes an advanced emulator for testing across various devices and OS versions. Android Studio's strong support for Gradle automation and version control improves productivity and ensures stable app deployment.

\subsubsection{Visual Studio Code}
Visual Studio Code is a lightweight and highly extensible code editor developed by Microsoft. It supports multiple programming languages including TypeScript, Python, and Kotlin through a rich extension ecosystem. The built-in IntelliSense, debugging tools, and terminal features make it suitable for both frontend and backend development. It offers real-time collaboration via Live Share, allowing developers to work together effectively across different environments. VS Code's flexibility and integration with GitHub and Docker make it an ideal all-purpose tool for modern software projects.

\subsubsection{GitHub}
GitHub is a cloud-based version control and collaboration platform built on Git. It enables developers to manage code efficiently, track revisions, and collaborate across teams through pull requests and branches. Its project management tools, including issues and boards, help maintain workflow organization. GitHub also provides CI/CD pipeline integration and security scanning features for automated testing and code quality assurance. The platform's global community and open-source ecosystem promote collaboration and innovation across diverse development teams.

\subsubsection{Figma}
Figma is a web-based design and prototyping tool used to create user interfaces and interactive prototypes. It supports real-time collaboration, allowing multiple designers and developers to work simultaneously. Its reusable component system and responsive design capabilities streamline the UI/UX workflow. Figma's browser-based nature eliminates installation issues and ensures cross-platform accessibility. The tool's integration with design systems and plugins enhances team efficiency and visual consistency across products.

\subsubsection{Notion}
Notion is an all-in-one workspace that combines documentation, task management, and project collaboration. It allows teams to organize notes, schedules, and resources in a centralized environment. Customizable templates and database features make it adaptable to different workflows. Notion supports real-time collaboration and version tracking, reducing communication gaps between developers and designers. Its flexibility and minimal interface help maintain clear documentation throughout all stages of project development.

\subsubsection{PostgreSQL}
PostgreSQL is an open-source relational database management system known for its robustness and compliance with SQL standards. It provides advanced features such as transactions, indexing, and JSON support for hybrid data structures. The database ensures high data integrity and fault tolerance, making it suitable for mission-critical systems. PostgreSQL's scalability and strong community support enable continuous improvements and long-term stability. Its compatibility with cloud services and ORMs makes it a preferred choice for modern data-driven applications.

\subsubsection{Docker}
Docker is a containerization platform that enables applications to run consistently across different environments. It packages code and dependencies into lightweight containers that improve scalability and deployment speed. Docker eliminates environment inconsistencies, simplifying development and testing. Its support for microservices architecture allows modular and efficient system design. The platform's integration with orchestration tools like Kubernetes makes it a standard for modern cloud-native applications.

\subsubsection{ChatGPT}
ChatGPT is an advanced conversational AI model developed by OpenAI, accessible through the OpenAI API for integration into software systems. It provides natural language understanding and generation capabilities that assist developers in automating documentation, debugging, and code generation tasks. Through the API, ChatGPT can be embedded directly into applications, enabling intelligent interactions such as question answering, summarization, and contextual reasoning. It supports multiple programming languages and frameworks, making it highly adaptable to various development environments. The API's scalability and pay-per-use pricing structure allow flexible usage without the need for dedicated infrastructure. By leveraging the GPT-5 architecture, ChatGPT delivers reliable, context-aware responses that improve development efficiency and enhance user experience in interactive systems.

\subsubsection{Gemini}
Gemini is Google's advanced AI model designed for reasoning, coding assistance, and data analysis. It provides multimodal capabilities, allowing text, image, and structured data processing within a single framework. The model assists developers with intelligent suggestions, code generation, and system optimization. Gemini's adaptability and integration with Google's ecosystem make it a valuable tool for research and advanced development workflows.

\section{Specification}

\subsection{Splashing Page}

When the application is launched, a dedicated splash screen is displayed for approximately one to two seconds to prevent users from seeing an empty or uninitialized interface during data loading. This screen ensures a smooth and visually coherent startup experience by masking initial delays while core resources, user session data, and essential UI components are being prepared in the background. Once loading is complete, the app automatically transitions to the main interface without requiring any user interaction, creating a seamless and polished first impression.

\subsection{Sign Up Page}

\subsubsection{Email Registration}
Users enter a valid email address as their primary account identifier. The system validates the email format in real-time and displays an error message if the format is incorrect. When users submit the registration form, the system checks whether the email is already registered in the database. If the email is already in use, a clear message informs the user and suggests logging in instead. The email field cannot be changed after initial account creation and serves as the permanent login identifier.

\subsubsection{Password Creation}
Users create a password with a minimum length of six characters. The password field displays asterisks by default to protect privacy, and a toggle button allows users to reveal or hide the entered text. Below the password field, a confirmation field requires users to re-enter the same password to prevent typos. If the two passwords do not match, the system displays a warning message and prevents submission. A brief hint below the field reminds users of the minimum character requirement.

\subsubsection{Personal Information Input}
Users provide their family name and given name in separate fields, both of which are required for account creation. The birth date must be entered in YYYY-MM-DD format, and the system validates that users are at least twelve years old before allowing registration. Users select their gender from three options: Male, Female, or Other. An optional phone number field accepts Korean mobile numbers and automatically normalizes the format by removing hyphens and spaces. All personal information can be edited later through the profile management page.

\subsubsection{SNS Sign Up}
Users can register using Google, Kakao, or Naver accounts by clicking the corresponding button on the registration page. After selecting a provider, users are redirected to the provider's consent screen where they approve sharing basic profile information such as email, name, and profile image. Upon returning to the application, the registration form pre-fills the email field with the authenticated email address, which becomes read-only. Users complete the registration by entering their family name, given name, birth date, and gender. No password is required for social login accounts, as authentication is handled by the OAuth provider.

\subsubsection{Registration Completion}
A brief review step shows the entered information except the password so users can check it once more. Users agree to the terms and privacy policy by ticking checkboxes and tap Create account. A success message appears and the app moves to the login page automatically. If the network fails the page keeps the inputs and shows a Try again button. Clear guidance is shown for any error so users can finish without confusion.

\subsection{Sign In Page}

\subsubsection{Local Sign In}

\paragraph{Success}
Users enter their ID and password, which are hidden with star symbols and can be briefly shown by tapping the Show button. After pressing Sign In, if the information matches, the app opens the main page.

\paragraph{Wrong Password}
If the entered password is incorrect, the field border turns red, and a clear message appears asking the user to try again. The ID stays on the screen so users only reenter the password. A small hint message appears if Caps Lock is on, and the Forgot Password link guides users directly to reset. The screen limits repeated failed attempts and shows a short wait message before retrying.

\paragraph{Password Recovery}
Users who forget their password can reset it through a secure three-step verification process using email-based authentication. The process ensures account security while providing a straightforward recovery experience.

\begin{enumerate}
\item \textbf{Email Verification Request}: Users enter their registered email address on the password recovery page. After submitting, the system sends a six-digit verification code to the provided email address. A confirmation message informs users that the code has been sent and instructs them to check their inbox. If the email is not registered, the system displays a generic success message to avoid revealing account existence. If the email belongs to a social login account, the system displays a specific error message explaining that password reset is not available and directs the user to log in with their provider.

\item \textbf{Code Verification}: Users enter the six-digit code received via email into the verification field. The code remains valid for ten minutes, and a countdown timer displays the remaining time. If users do not receive the code, they can click a "Resend Code" button after a brief waiting period. When the code is submitted, the system validates it against the stored value and checks the expiration time. If the code is correct and not expired, users proceed to the password reset step. If the code is incorrect, expired, or already used, an error message appears with appropriate instructions.

\item \textbf{Password Reset}: Users create a new password that meets the minimum six-character requirement. A confirmation field requires users to re-enter the password to prevent typos. The password field includes a toggle button to show or hide the entered text. After submitting, the system updates the stored password and displays a success message. Users are then automatically redirected to the login page where they can sign in with their new credentials.
\end{enumerate}

\subsubsection{SNS Sign In}
Users can sign in with social accounts such as Google, Naver, or Kakao by tapping the provider button. After confirming account access on the provider's page, users return to the app where basic information like name and email are automatically filled. If the account is new, the app asks for additional fields such as email address or age before completion. When sign-in succeeds, the app moves to the main page and shows a welcome notification.

\subsection{Main Page}

\subsubsection{Preference Survey}
When users log in for the first time, a full-screen survey overlay appears before the home page content. The survey consists of three steps: scent preferences, lighting preferences, and music preferences. Each step presents multiple-choice questions with visual icons representing each option. Users can select up to three favorite items and any number of disliked items. Navigation buttons at the bottom allow users to move between steps or skip the survey entirely. If users skip, default preferences are applied automatically. After completing or skipping the survey, the overlay closes and does not appear again unless users reset their preferences.

\subsubsection{Mood Dashboard}
The mood dashboard occupies the upper section of the home page and displays the current mood name, scent type, lighting color, and music genre. Each element is presented with descriptive labels and visual indicators such as color swatches for lighting. Four control buttons allow users to change the entire mood, scent only, song only, or color only. When users click any of these buttons, a selection interface appears with available options. After making a selection, the dashboard updates to reflect the new mood settings, and all connected devices adjust accordingly.

\subsubsection{Device Management}

\paragraph{Device Type Selection}
Clicking the "Add Device" button opens a modal displaying four device type options: Manager, Light, Scent, and Speaker. Each option is represented by a large card with an icon, label, and brief description of its purpose. Users select a type by clicking the corresponding card, which then proceeds to the naming step. The modal can be closed at any time by clicking the background overlay or a close button.

\paragraph{Device Name Setup}
After selecting a type, the modal displays a text input field where users can enter a custom device name. A default name is pre-filled based on the device type and the number of existing devices of that type. For example, the first light device defaults to "Light \#1" and the second to "Light \#2". Users can accept the default name or replace it with a custom value. A "Confirm" button submits the registration, while a "Cancel" button returns to the type selection screen.

\paragraph{Device Addition}
When users confirm the device name, the modal closes and the new device immediately appears in the device grid. A brief success notification displays the device name to confirm the addition. The newly added device starts in the powered-off state with full battery. Users can immediately click the device card to expand it and adjust settings.

\paragraph{Device Deletion}
Each expanded device card includes a delete button in the footer section. When clicked, a confirmation dialog appears asking users to verify the deletion. The dialog displays the device name and warns that this action cannot be undone. If users confirm, the device is removed from the grid and all associated data is deleted. If users cancel, the dialog closes and no changes occur.

\subsubsection{AI-Based Recommended Mood}
A prominent "Generate Mood" button triggers the AI-based mood recommendation system. When clicked, a loading indicator appears with a message explaining that the system is analyzing biometric data. The generation process considers recent stress levels, sleep quality, emotion events, weather conditions, and user preferences. After processing, the dashboard updates with the newly generated mood, and a notification displays the mood name. Users can regenerate the mood at any time by clicking the button again.

\subsubsection{Mood Routine Management}
This section allows users to schedule mood changes automatically throughout the day. A simple list view displays entries such as 08:00–Relax Mode, 14:00–Focus Mode, and 22:00–Sleep Mode. Users tap Add Routine to create new ones, selecting a start time, end time, and mood. Each routine has an on/off toggle for quick control and can be reordered with drag-and-drop. Notifications appear five minutes before each scheduled change to remind users.

\subsection{Mood Page}

\subsubsection{Mood Segment Display}
The Mood page displays all saved mood segments in a grid layout. Each segment is represented by a card showing the segment name, creation timestamp, and a visual preview of its components including lighting color swatch, scent type icon, and music genre label. Segments are sorted by creation date with the most recent appearing first. Users can scroll through the list to view all available segments. A search box at the top of the page allows users to filter segments by name or component type.

\subsubsection{Stream Creation}
Users can create mood streams by grouping multiple segments together. A "Create Stream" button opens a modal where users can select segments to include in the stream. The modal displays all available segments with checkboxes, and users can select as many segments as desired. After selecting segments, users enter a name for the stream and click "Create" to save it. The newly created stream appears in a separate "Streams" section on the Mood page, displaying the stream name, number of segments, and total duration.

\subsubsection{Segment and Stream Application}
Users can apply a mood segment or stream to their devices by double-clicking the corresponding card. A confirmation dialog appears asking "Do you want to apply this mood?" with options to confirm or cancel. If users confirm, the dialog displays a second choice: "Replace current segment" or "Replace entire stream". Selecting "Replace current segment" applies only the selected segment to all devices. Selecting "Replace entire stream" queues all segments in the stream to play sequentially. After confirmation, the dialog closes and the selected mood is applied immediately, updating all connected devices with the new settings.

\subsubsection{Segment and Stream Management}
Each segment and stream card includes a menu button that reveals additional options. The menu includes "Rename" which opens a text input dialog for changing the item name, and "Delete" which removes the item permanently. When users select "Delete" for a stream, a warning appears explaining that deleting the stream does not delete the individual segments it contains. For segments that belong to one or more streams, the delete confirmation lists which streams will be affected. Users must confirm the deletion before it proceeds, and a success notification appears after the item is removed.

\subsubsection{Stream Organization}
Users can reorganize segments within a stream by clicking "Edit Stream" from the stream's menu. This opens a modal displaying all segments in the stream as draggable cards. Users can drag segments up or down to reorder them, and the new order is saved when users click "Save Order". The stream's total duration updates automatically as segments are added, removed, or reordered. Users can also remove individual segments from the stream without deleting them entirely by clicking a small "x" icon on each segment card within the edit modal.

\subsection{My Page}

\subsubsection{Profile Display}
The profile page displays the user's current information including email, family name, given name, birth date, gender, phone number, and profile image. The email field is read-only and cannot be changed. If the account uses social login, a badge displays the provider name such as "Connected with Google". An "Edit Profile" button at the top of the page enables editing mode for modifiable fields.

\subsubsection{Profile Editing}
Clicking the "Edit Profile" button transforms display fields into editable inputs. Users can upload a new profile image by clicking the current image and selecting a file. Text fields for family name, given name, phone number become active for editing. The birth date field opens a date picker when clicked. The gender field displays a dropdown menu with three options. A "Save Changes" button submits the modifications, while a "Cancel" button discards changes and returns to display mode. After saving, a success message appears and the page refreshes to show the updated information.

\subsubsection{Account Deletion}
A "Delete Account" button appears in the danger zone section of the account settings. When clicked, a confirmation dialog warns that this action is permanent and will delete all associated data including devices, presets, and preferences. Users must confirm by clicking "Delete" in the dialog or cancel to abort. If users confirm deletion, they are immediately logged out and redirected to the login page with a notification that the account has been permanently deleted.

\subsubsection{Logout}
The Logout button signs the user out of the current account immediately. When the button is clicked, app returns to the sign-in page and clears saved session data for security.

\subsubsection{Q\&A}
The Q\&A page displays a list of frequently asked questions organized in an accordion format. Each question appears as a clickable header that expands to reveal the answer. Topics include how the mood recommendation system works, what biometric data is collected, how to customize presets, and how to add devices. Users can expand multiple questions simultaneously to compare answers. The page is accessible through a link in the Mypage menu.

\subsubsection{1:1 Inquiry}
The inquiry page provides a form where users can submit support questions or feedback. Users enter a subject line and a detailed message in a multi-line text area. Both fields are required before submission. After submitting, a success message confirms that the inquiry has been received and explains that a response will be provided via email. The form clears after successful submission, allowing users to submit another inquiry if needed.

\subsubsection{Privacy Policy}
The privacy policy page displays comprehensive information about data collection, storage, usage, and user rights. The content is organized into sections with clear headings covering topics such as biometric data collection, third-party service integration, data retention policies, and account deletion procedures. Users can scroll through the entire policy without any interactive elements. The page is accessible through a link in the Mypage menu.

\section{Architecture Design \& Implementation}

\subsection{Overall Architecture}

The Mood Manager architecture is composed of six primary modules: the WearOS Client, the Cloud Firestore Sync Layer, the ML Event Classification Server, the Web Application, the Backend Service Layer, and the AI Inference Module. Together, these components form an end-to-end pipeline that collects multimodal signals from the user, processes them through cloud-connected services, and generates personalized mood outputs through AI reasoning. Each module plays an essential role in ensuring that biometric, acoustic, and contextual information flows smoothly across the system.

The first module is the WearOS Client, implemented as a Kotlin-based native application that operates continuously through a foreground service. It gathers biometric signals such as heart rate, HRV, stress indicators, and movement counts, while also capturing short audio segments for emotional context. This module serves as the system's data origin, formatting and transmitting periodic measurements to Cloud Firestore with strict timing to maintain consistency for downstream processing.

The second module is the Cloud Firestore Sync Layer, which acts as a real-time communication bridge between the wearable device, the ML service, and the web application. Firestore stores biometric samples and audio-event data under structured user collections, enabling event-driven updates and a stable time-series dataset. This layer removes the need for direct device–server links and supports the continuous flow of information throughout the system.

The third module, the ML Event Classification Server, is deployed on AWS and processes audio events sent from Firestore. It retrieves Base64-encoded WAV inputs, applies inference logic to classify laughter, sigh, or noise, and posts validated results back to the Web Application. By running on a containerized environment, the ML server maintains scalable event processing while ensuring that multimodal emotional signals are available for integrated mood inference.

At the center of the workflow is the Web Application, built with Next.js, React, and TailwindCSS. It collects biometric samples from Firestore, retrieves classification results from the ML server, merges them with user preferences and weather information, and orchestrates the logic required to determine mood attributes. This module also delivers the primary interface for mood visualization, device control, and user management, acting as the core processing and interaction layer of the system.

Supporting these operations is the Backend Service Layer, implemented using Node.js and Prisma. It manages database access, validates operations, and provides structured endpoints for authentication, preference management, device operations, and mood updates. This module ensures consistent interaction between the Web Application and the persistent storage, while also enabling clean integration points for advanced analytics and future system extensions.

The final component is the AI Inference Module, which uses OpenAI's few-shot prompting to convert processed biometric and contextual inputs into structured mood profiles. It generates recommendations for lighting, scent, and sound, along with a descriptive mood name that reflects the inferred emotional state. This module encapsulates the system's adaptive reasoning and provides the core intelligence behind the personalized mood experience.

\subsection{Directory Organization}

\onecolumn

\begin{longtable}{|p{5cm}|p{8cm}|}
\caption{Directory Organization - BACKEND} \\
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endfirsthead
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endhead
\hline
\endfoot
/src/app/api/auth/[...nextauth] & route.ts \\
\hline
/src/app/api/auth/register & route.ts \\
\hline
/src/app/api/auth/profile & route.ts \\
\hline
/src/app/api/auth/account & route.ts \\
\hline
/src/app/api/auth/survey-status & route.ts \\
\hline
/src/app/api/auth/forgot-password & route.ts \\
\hline
/src/app/api/auth/verify-reset-code & route.ts \\
\hline
/src/app/api/auth/reset-password & route.ts \\
\hline
/src/app/api/devices & route.ts \\
\hline
/src/app/api/devices/[deviceId] & route.ts \\
\hline
/src/app/api/devices/[deviceId]/power & route.ts \\
\hline
/src/app/api/devices/[deviceId]/name & route.ts \\
\hline
/src/app/api/devices/[deviceId]/scent-interval & route.ts \\
\hline
/src/app/api/moods/current & route.ts, scent/route.ts, song/route.ts, color/route.ts, refresh/route.ts \\
\hline
/src/app/api/inquiry & route.ts \\
\hline
/src/app/api/preferences & route.ts \\
\hline
/src/app/api/preprocessing & route.ts \\
\hline
/src/app/api/sleep/today & route.ts \\
\hline
/src/app/api/sleep/generate & route.ts \\
\hline
/src/app/api/ml/emotion & route.ts \\
\hline
/backend/listener & periodicListener.ts \\
\hline
/backend/jobs & preprocessPeriodic.ts, processEmotion.ts, calcTodaySleepScore.ts, fetchTodayPeriodicRaw.ts, fetchTodaySleepRaw.ts, generateDummySleepData.ts, sleepSessionDetector.ts, sleepWakeClassifier.ts \\
\hline
/backend/cache & periodicCache.ts \\
\hline
/prisma & schema.prisma, seed.ts, update-seed-data.ts \\
\hline
\end{longtable}

\begin{longtable}{|p{5cm}|p{8cm}|}
\caption{Directory Organization - FRONTEND} \\
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endfirsthead
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endhead
\hline
\endfoot
/src/app & layout.tsx, page.tsx, not-found.tsx \\
\hline
/src/app/(auth) & login/page.tsx, register/page.tsx, forgot-password/page.tsx \\
\hline
/src/app/(auth)/register/components & RegisterForm.tsx, EmailSection.tsx, PasswordSection.tsx, ConfirmPasswordSection.tsx, PersonalInfoSection.tsx, BirthDateGenderSection.tsx \\
\hline
/src/app/(auth)/register/hooks & useRegisterForm.ts \\
\hline
/src/app/(auth)/forgot-password/components & ForgotPasswordSteps.tsx \\
\hline
/src/app/(auth)/forgot-password/hooks & useForgotPassword.ts \\
\hline
/src/app/(main)/home & page.tsx \\
\hline
/src/app/(main)/home/components & HomeContent.tsx \\
\hline
/src/app/(main)/home/components/MoodDashboard & MoodDashboard.tsx \\
\hline
/src/app/(main)/home/components/MoodDashboard/components & MoodHeader.tsx, AlbumSection.tsx, ScentControl.tsx, MusicControls.tsx, MoodDuration.tsx, HeartAnimation.tsx \\
\hline
/src/app/(main)/home/components/MoodDashboard/hooks & useMoodDashboard.ts, useMoodColors.ts, useHeartAnimation.ts, useSegmentSelector.ts \\
\hline
/src/app/(main)/home/components/Device & DeviceGrid.tsx, DeviceCardSmall.tsx, DeviceCardExpanded.tsx, AddDeviceCard.tsx, DeviceAddModal.tsx, DeviceTypeSelectModal.tsx, DeviceNameInputModal.tsx, DeviceDeleteModal.tsx \\
\hline
/src/app/(main)/home/components/Device/components & DeviceControls.tsx, DeviceNameEditor.tsx \\
\hline
/src/app/(main)/home/components/Device/hooks & useDeviceCard.ts, useDeviceHandlers.ts \\
\hline
/src/app/(main)/home/components/Device/utils & deviceUtils.tsx \\
\hline
/src/app/(main)/home/components/SurveyOverlay & SurveyOverlay.tsx \\
\hline
/src/app/(main)/mypage & page.tsx \\
\hline
/src/app/(main)/mypage/components & MenuSection.tsx, ProfileSection.tsx, DeleteAccountModal.tsx \\
\hline
/src/components & ErrorBoundary.tsx, SessionProvider.tsx \\
\hline
/src/components/navigation & TopNav.tsx, BottomNav.tsx \\
\hline
/src/components/ui & Skeleton.tsx, ScentBackground.tsx \\
\hline
\end{longtable}

\begin{longtable}{|p{5cm}|p{8cm}|}
\caption{Directory Organization - CORE MODULES} \\
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endfirsthead
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endhead
\hline
\endfoot
/src/lib/auth & password.ts, session.ts \\
\hline
/src/lib/firebase & client.ts, admin.ts \\
\hline
/src/lib/openai & index.ts, formatPayload.ts \\
\hline
/src/lib/preprocessing & preprocess.ts, prepareOpenAIInput.ts \\
\hline
/src/lib/sleep & detectSleepStage.ts, calculateSleepScore.ts, calculateDailySleepScore.ts, utils.ts \\
\hline
/src/lib/stress & calculateStressIndex.ts, utils.ts \\
\hline
/src/lib/weather & index.ts, fetchWeather.ts, mapGrid.ts \\
\hline
/src/lib/preferences & getPreferences.ts, updatePreferences.ts, mapPreferencesForAI.ts, preferenceUtils.ts \\
\hline
/src/lib/moodSignals & fetchDailySignals.ts \\
\hline
/src/lib/utils & validation.ts, time.ts \\
\hline
/src/hooks & useMood.ts, useSurvey.ts, useMoodStream.ts, useBackgroundParams.ts \\
\hline
/src/types & mood.ts, device.ts, preset.ts, emotion.ts, survey.ts \\
\hline
\end{longtable}

\begin{longtable}{|p{5cm}|p{8cm}|}
\caption{Directory Organization - ML} \\
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endfirsthead
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endhead
\hline
\endfoot
ML/apps/ml\_classification & predict.py, sigh\_laugh\_neg\_cls.py, convert\_onnx.py, dataset\_download.py, ml\_settings.py, fix\_config.py, requirements\_docker.txt \\
\hline
ML/apps/ml\_classification/onnx\_model & model\_quantized.onnx, config.json, ort\_config.json, preprocessor\_config.json \\
\hline
ML/apps/ml\_classification/saved\_model & config.json, preprocessor\_config.json \\
\hline
\end{longtable}

\begin{longtable}{|p{5cm}|p{8cm}|}
\caption{Directory Organization - WearOS} \\
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endfirsthead
\hline
\textbf{Directory} & \textbf{File Name} \\
\hline
\endhead
\hline
\endfoot
Watch & build.gradle.kts, gradle.properties, settings.gradle.kts \\
\hline
Watch/app & build.gradle.kts, lint.xml \\
\hline
Watch/app/src/main & AndroidManifest.xml \\
\hline
Watch/app/src/main/java/com/moodmanager/watch/presentation & MainActivity.kt, AudioEventService.kt, PeriodicDataService.kt, FirebaseViewModel.kt \\
\hline
Watch/app/src/main/java/com/moodmanager/watch/presentation/theme & Theme.kt \\
\hline
Watch/app/src/main/res & drawable/, mipmap-*, values/, values-round/ \\
\hline
\end{longtable}

\twocolumn

\subsection{Module 1: Backend}

\subsubsection{Purpose}
The Backend Module of Mood Manager provides the core server-side logic responsible for authentication, device management, mood retrieval, data preprocessing, and system-wide orchestration. Acting as the integration point between the Next.js frontend, Firestore data streams, and the AI inference module, it centralizes application logic within structured API routes to ensure consistency, security, and reliable communication throughout the system. In addition to handling requests, the backend manages time-series aggregation, biometric preprocessing, and synchronization of user state for mood inference. Listener, job, and cache components support continuous processing workflows—from WearOS data ingestion to database persistence—forming the operational backbone that enables system stability, scalability, and coherent data flow.

\subsubsection{Functionality}
The Backend Module implements comprehensive REST-style API endpoints for authentication, password recovery, survey status management, device control, and mood customization. These APIs support power toggling, scent interval adjustments, mood refreshing, and mood attribute updates, while additional routes process biometric inputs, compute sleep metrics, and accept emotion classification results from external ML services. Listener modules and scheduled jobs handle periodic physiological data uploaded from WearOS, performing sleep detection, stress calculation, normalization, and daily metric generation. Cache utilities reduce Firestore load by storing intermediate biometric states, and Prisma serves as the database interface for structured access to user profiles, device configurations, and mood history records.

\subsubsection{Location of Source Code}
https://github.com/Ma-Hyekjin/mood-manager/src

\subsubsection{Class Components}

\begin{itemize}
\item auth/[...nextauth] : route.ts : Handles NextAuth session initialization, OAuth login, and session callbacks.
\item auth/register : route.ts : Processes new user registration and validation.
\item auth/profile : route.ts : Returns and updates authenticated user profile data.
\item auth/account : route.ts : Manages account deletion and sensitive account settings.
\item auth/survey-status : route.ts : Checks whether the user has completed the preference survey.
\item auth/forgot-password : route.ts : Sends reset codes to user email during password recovery.
\item auth/verify-reset-code : route.ts : Validates the reset code submitted by the user.
\item auth/reset-password : route.ts : Resets the user's password upon successful verification.
\item devices : route.ts : Lists all registered devices for the user.
\item devices/[deviceId] : route.ts : Retrieves or deletes a specific device.
\item devices/[deviceId]/power : route.ts : Toggles device power state.
\item devices/[deviceId]/name : route.ts : Updates device name.
\item devices/[deviceId]/scent-interval : route.ts : Updates fragrance output interval.
\item moods/current : route.ts : Returns the current inferred mood profile.
\item moods/current/scent : route.ts : Updates the scent component of the current mood.
\item moods/current/song : route.ts : Updates music selection within the mood.
\item moods/current/color : route.ts : Updates lighting color attributes.
\item moods/current/refresh : route.ts : Forces regeneration of a new mood recommendation.
\item inquiry : route.ts : Submits user inquiries for customer support.
\item preferences : route.ts : Gets or updates user preference data.
\item preprocessing : route.ts : Preprocesses periodic biometric data for mood inference.
\item sleep/today : route.ts : Retrieves today's computed sleep metrics.
\item sleep/generate : route.ts : Generates synthetic sleep data for testing.
\item ml/emotion : route.ts : Receives emotion classification results from the ML microservice.
\item periodicListener.ts : Listens for new periodic biometric entries and triggers preprocessing
\item preprocessPeriodic.ts : Converts raw biometric data into normalized metrics.
\item processEmotion.ts : Integrates emotion data into the mood pipeline.
\item calcTodaySleepScore.ts : Computes daily sleep score using HRV and movement metrics.
\item fetchTodayPeriodicRaw.ts : Fetches today's raw periodic biometric dataset.
\item fetchTodaySleepRaw.ts : Fetches raw sleep sessions for processing.
\item generateDummySleepData.ts : Creates mock sleep datasets for debugging.
\item sleepSessionDetector.ts : Detects sleep intervals based on movement and HR patterns.
\item sleepWakeClassifier.ts : Classifies segments into sleep or wake states.
\item periodicCache.ts : Provides in-memory caching for recent biometric computations.
\item schema.prisma : Defines SQL schema for users, devices, preferences, and mood logs.
\item seed.ts : Populates the initial database with baseline test data.
\item update-seed-data.ts : Updates or extends seeded database entries.
\end{itemize}

\subsection{Module 2: Frontend}

\subsubsection{Purpose}
The Frontend Module of Mood Manager provides the user-facing interface through which individuals can authenticate, view their personalized mood state, manage smart-home devices, and update preferences in a clear and responsive manner. Developed using Next.js with a focus on usability and consistent design, the frontend transforms complex biometric and AI-generated insights into intuitive visual elements. It communicates with the backend APIs, renders real-time mood updates, manages user sessions, and drives various guided flows such as onboarding, survey collection, mood presentation, and device control. As the presentation layer of the system, the frontend ensures that every interaction—from mood adjustments to profile edits—operates smoothly and remains accessible across devices.

\subsubsection{Functionality}
The Frontend Module delivers a complete set of interface-level features, including registration, login, password recovery, and survey collection for preference initialization. It displays real-time mood information such as scent, music, lighting, and duration, and provides device management tools for adding, renaming, deleting, and configuring smart-home devices. It also supports user profile management, account deletion, navigation layout rendering, and visual effects like animated heartbeats and dynamic mood backgrounds. Custom hooks abstract the management of mood states, surveys, animations, and device operations, while shared UI components ensure uniform styling and interactions throughout the system. The frontend acts as the hub for user interaction, coordinating data flow between backend logic, preprocessing workflows, and AI inference results.

\subsubsection{Location of Source Code}
https://github.com/Ma-Hyekjin/mood-manager/src

\subsubsection{Class Components}

\begin{itemize}
\item layout.tsx : Defines the global layout wrapping all pages.
\item page.tsx : Application entry splash page.
\item not-found.tsx : Custom 404 error rendering.
\item login/page.tsx : Login interface for user authentication.
\item register/page.tsx : User registration page.
\item forgot-password/page.tsx : Password recovery request page.
\item RegisterForm.tsx : Controller for the registration form flow.
\item EmailSection.tsx : Email field and validation UI.
\item PasswordSection.tsx : Password creation interface.
\item ConfirmPasswordSection.tsx : Password confirmation form.
\item PersonalInfoSection.tsx : Name and personal information fields.
\item BirthDateGenderSection.tsx : Birth date selector and gender input.
\item useRegisterForm.ts : Registration form logic and validation.
\item ForgotPasswordSteps.tsx : Multi-step UI for password reset.
\item useForgotPassword.ts : Hook controlling password recovery flow.
\item home/page.tsx : Main dashboard displaying inferred mood and devices.
\item HomeContent.tsx : Primary container for dashboard contents.
\item MoodDashboard.tsx : Wrapper for mood presentation UI.
\item MoodHeader.tsx : Displays current mood name and short description.
\item AlbumSection.tsx : Music artwork and audio-related visuals.
\item ScentControl.tsx : Interface for adjusting scent intensity.
\item MusicControls.tsx : Controls for play, next song, and selection.
\item MoodDuration.tsx : Remaining duration for the current mood.
\item HeartAnimation.tsx : Animated heart visual synced with mood.
\item useMoodDashboard.ts : Fetches and manages the overall mood state.
\item useMoodColors.ts : Computes theme colors based on mood attributes.
\item useHeartAnimation.ts : Animation controller for heartbeat visuals.
\item useSegmentSelector.ts : Handles selection logic for segmented UI.
\item DeviceGrid.tsx : Grid view displaying all devices.
\item DeviceCardSmall.tsx : Compact visualization for each device.
\item DeviceCardExpanded.tsx : Expanded card with full controls.
\item AddDeviceCard.tsx : Entry component for adding new devices.
\item DeviceAddModal.tsx : Modal for creating a new device.
\item DeviceTypeSelectModal.tsx : Modal for choosing device type.
\item DeviceNameInputModal.tsx : Modal for device naming.
\item DeviceDeleteModal.tsx : Confirmation modal for device removal.
\item DeviceControls.tsx : Power, scent interval, and control buttons.
\item DeviceNameEditor.tsx : Inline device rename interface.
\item useDeviceCard.ts : Logic for device card interactions.
\item useDeviceHandlers.ts : Encapsulates API operations for devices.
\item deviceUtils.tsx : Utility functions used across device components.
\item SurveyOverlay.tsx : Popup survey for collecting user preferences.
\item mypage/page.tsx : User profile main page.
\item MenuSection.tsx : Navigation menu inside MyPage.
\item ProfileSection.tsx : User profile information and editing UI.
\item DeleteAccountModal.tsx : Account deletion confirmation modal.
\item ErrorBoundary.tsx : Catches and displays global errors.
\item SessionProvider.tsx : Wraps application with authentication context.
\item TopNav.tsx, BottomNav.tsx : Navigation bars for the app.
\item Skeleton.tsx : Loading placeholder component.
\item ScentBackground.tsx : Dynamic visual background reflecting scent category.
\end{itemize}

\subsection{Module 3: Core Modules}

\subsubsection{Purpose}
The Core Modules of Mood Manager provide the foundational logic shared across all parts of the system, including authentication utilities, Firebase access layers, OpenAI integration, biometric preprocessing, sleep and stress analytics, weather retrieval, preference mapping, and global utility functions. These modules abstract low-level operations into reusable building blocks, enabling the frontend and backend to maintain clean, modular structures. By centralizing cross-cutting functionality such as data formatting, AI prompt preparation, physiological scoring logic, and input validation, the Core Modules ensure consistency, reduce duplication, and provide a stable computational base for the entire system's operations. This layer serves as the system's internal backbone, supporting computation-heavy tasks and guaranteeing coherent behavior across different application workflows.

\subsubsection{Functionality}
The Core Modules offer a wide spectrum of reusable functionalities essential for mood inference, device interaction, and user state management. Authentication helpers securely manage password hashing and session handling, while Firebase clients handle client-side and admin-level Firestore communication. OpenAI modules prepare structured prompts, format inputs, and trigger LLM-based mood generation. Preprocessing utilities convert raw biometric streams into normalized metrics, and sleep/stress algorithms compute meaningful indicators from heart rate, HRV, and movement patterns. Weather and preference modules enrich context with environmental and personal information, while moodSignal utilities aggregate daily emotional events. Shared validation, time utilities, hooks, and TypeScript types standardize logic across pages and API routes. Collectively, these modules maintain the computational, analytical, and structural consistency that the higher-level system relies on.

\subsubsection{Location of Source Code}
https://github.com/Ma-Hyekjin/mood-manager/src

\subsubsection{Class Components}

\begin{itemize}
\item password.ts : Hashes and verifies passwords.
\item session.ts : Manages session creation and validation.
\item client.ts : Handles client-side Firebase initialization.
\item admin.ts : Provides server-side Firestore admin access.
\item index.ts : Sends requests to OpenAI for mood inference.
\item formatPayload.ts : Formats data into AI-ready JSON.
\item preprocess.ts : Normalizes biometric raw data.
\item prepareOpenAIInput.ts : Generates structured input for LLM mood generation.
\item detectSleepStage.ts : Identifies sleep stages from physiological patterns.
\item calculateSleepScore.ts : Computes score for a sleep session.
\item calculateDailySleepScore.ts : Aggregates daily sleep score.
\item utils.ts : Shared sleep-related math and helpers.
\item calculateStressIndex.ts : Produces stress index from HRV and movement.
\item utils.ts : HRV-related helper functions.
\item index.ts : Main module for retrieving weather.
\item fetchWeather.ts : Queries external weather APIs.
\item mapGrid.ts : Maps coordinates to grid-based weather models.
\item getPreferences.ts : Retrieves stored user preferences.
\item updatePreferences.ts : Updates user preference dataset.
\item mapPreferencesForAI.ts : Converts preferences into AI-weighted values.
\item preferenceUtils.ts : Helper functions for preference logic.
\item fetchDailySignals.ts : Retrieves daily emotional events for mood inference.
\item validation.ts : Common validation helpers.
\item time.ts : Time formatting and manipulation utilities.
\item useMood.ts : Fetches and updates mood states.
\item useSurvey.ts : Manages survey completion and state.
\item useMoodStream.ts : Handles real-time mood updates.
\item useBackgroundParams.ts : Controls dashboard background effects.
\item mood.ts : Defines mood-related data structures.
\item device.ts : Defines device-related types.
\item preset.ts : Defines environment preset types.
\item emotion.ts : Defines emotion classification types.
\item survey.ts : Defines survey and preference types.
\end{itemize}

\subsection{Module 4: ML}

\subsubsection{Purpose}
The ML Module of Mood Manager provides the machine learning infrastructure responsible for classifying audio-based emotional cues, specifically laughter, sighs, and neutral sounds. This module operates as an external microservice that processes WAV/Base64 audio streams uploaded from WearOS, transforming them into discrete emotional events used by the backend to refine mood inference. Built with lightweight ONNX models for efficient execution, the ML Module ensures fast, reliable classification across devices and environments. It acts as the system's perceptual layer, extending biometric sensing with voice-based signals and supplying essential multimodal data that enhances personalization.

\subsubsection{Functionality}
The ML Module downloads, preprocesses, and classifies audio samples using trained neural networks exported in ONNX format. It handles dataset acquisition, model conversion, inference execution, and configuration management while providing a REST endpoint for returning classification results to the backend. Preprocessors standardize audio inputs, and model settings configure sampling rates, thresholds, and feature extraction. Saved models and quantized ONNX versions ensure optimized runtime performance. Configuration files allow reproducible model behavior, and utilities support dataset generation, ONNX export, and debugging in local or Dockerized environments. By converting raw acoustic signals into structured emotional labels, the ML Module contributes directly to the Mood Manager's contextual awareness.

\subsubsection{Location of Source Code}
https://github.com/Ma-Hyekjin/mood-manager/ML

\subsubsection{Class Components}

\begin{itemize}
\item predict.py : Runs inference on audio samples and returns predicted emotion class.
\item sigh\_laugh\_neg\_cls.py : Implements the main classifier for sigh/laugh/negative categories.
\item convert\_onnx.py : Converts trained PyTorch/TensorFlow models into ONNX format.
\item dataset\_download.py : Downloads and organizes external datasets for emotion classification.
\item ml\_settings.py : Configuration file defining model hyperparameters and thresholds.
\item fix\_config.py : Applies patches or adjustments to existing model configuration files.
\item requirements\_docker.txt : Lists dependencies for running ML service in Docker or isolated environments.
\item model\_quantized.onnx : Optimized ONNX model for runtime inference.
\item config.json : Model configuration parameters.
\item ort\_config.json : ONNX Runtime-specific configuration.
\item preprocessor\_config.json : Rules for audio preprocessing prior to model inference.
\item saved\_model/config.json : Full model configuration for reproducibility.
\item saved\_model/preprocessor\_config.json : Preprocessor settings for locally saved model.
\end{itemize}

\subsection{Module 5: WearOS}

\subsubsection{Purpose}
The WearOS Module of Mood Manager serves as the primary data acquisition layer, continuously collecting multimodal physiological and acoustic signals directly from the user's wrist-worn device. Implemented as a native Kotlin application for Wear OS, this module operates autonomously in the background through foreground services, ensuring uninterrupted data capture even when the device screen is off or the user is engaged in other activities. By integrating with Android's Health Connect API and native audio recording capabilities, the WearOS app gathers essential biometric metrics—including heart rate, heart rate variability, stress indices, and respiratory rate—alongside contextual audio events that indicate emotional states such as laughter or sighs. All collected data is transmitted in real time to Firebase Firestore, establishing a reliable and structured data stream that feeds into the backend preprocessing pipeline and ultimately drives AI-powered mood inference. As the foundational sensor layer of the Mood Manager ecosystem, the WearOS module bridges the gap between raw physiological measurements and intelligent mood prediction, enabling the system to respond dynamically to the user's emotional and physical state throughout the day.

\subsubsection{Functionality}
The WearOS Module implements two independent foreground services that run continuously to ensure robust data collection without interruption. The PeriodicDataService queries the Health Connect API at regular intervals to retrieve biometric measurements such as instantaneous heart rate, heart rate variability metrics, stress levels derived from HRV analysis, and respiratory rate estimates. These data points are normalized, timestamped, and uploaded to the Firestore collection `users/{userId}/raw_periodic`, creating a time-series dataset that supports sleep detection, stress tracking, and daily wellness scoring. In parallel, the AudioEventService activates the device microphone every minute for a brief two-second recording window, capturing ambient sound and analyzing it for emotional cues. Using lightweight on-device heuristics based on audio amplitude and frequency characteristics, the service classifies detected events as laughter, sighs, or background noise, encoding the raw audio as Base64-encoded WAV and uploading validated events to `users/{userId}/raw_events`. The MainActivity coordinates permission management for sensitive data access—requesting notifications, body sensors, and audio recording permissions—and initializes both services upon app launch. All operations comply with Android's battery optimization and privacy constraints by leveraging foreground service notifications and maintaining minimal resource consumption. The WearOS app's data flows directly into the backend's real-time Firestore listeners, ensuring seamless integration with the broader Mood Manager architecture and enabling context-aware mood recommendations driven by continuous physiological and emotional monitoring.

\subsubsection{Location of Source Code}
https://github.com/Ma-Hyekjin/mood-manager/Watch

\subsubsection{Class Components}

\begin{itemize}
\item MainActivity.kt : Main activity that handles permission requests and starts foreground services.
\item AudioEventService.kt : Foreground service that records audio every minute and detects laughter/sigh events.
\item PeriodicDataService.kt : Foreground service that collects biometric data periodically from Health Connect API.
\item FirebaseViewModel.kt : ViewModel for managing Firebase Firestore data synchronization.
\item Theme.kt : Wear OS Material Design theme configuration.
\item AndroidManifest.xml : App configuration including permissions and service declarations.
\item build.gradle.kts : Gradle build configuration for Watch app module.
\item gradle.properties : Gradle project-wide properties.
\item settings.gradle.kts : Gradle settings for multi-module project structure.
\end{itemize}

\section{Use Case}

\subsection{Sign Up}
The user enters name, password, birthdate and gender to create a new account. Alternatively, users can sign up through social authentication (Google, Kakao, or Naver), which automatically fills in email and name. After entering information, they can proceed to the login page and attempt to log in with the newly created account.

\subsection{Sign In}
After signing up, the login page is displayed. User then enters their registered email and password to log in and access the system. Social login options are also available. The system implements rate limiting, allowing a maximum of 5 login attempts before locking the account for 15 minutes.

\subsection{Survey}
When the user first accesses the home page after registration, a survey overlay appears to set up mood preferences. The survey collects favorite scents, lighting colors, and music genres to personalize AI recommendations. Users can complete the survey or skip it to use default preferences.

\subsection{Device Registration}
The user clicks the "+ Add Device" card to open the device registration modal. The modal displays four device type options: Manager, Light, Speaker, and Scent. After selecting a device type and entering a name, the system registers the device and displays it in the device grid.

\subsection{Mood Dashboard}
The mood dashboard displays the current AI-generated mood with a creative name and circular album art showing the associated music track. The dashboard features a 30-minute mood stream timeline divided into 10 segments. Users can interact with the timeline by clicking individual segments to jump to different mood states.

\subsection{Refresh Mood}
The user clicks the refresh button to request a new AI mood recommendation. The system fetches latest biometric data from Firestore, preprocesses it, and calls OpenAI API to generate a new 30-minute mood stream. The dashboard updates with the new AI-generated mood name, music, scent, and lighting combination.

\subsection{Scent Control}
The user can control scent settings through the mood dashboard and device cards. Users can change fragrance type, adjust intensity level, and configure spray interval.

\subsubsection{Change Scent Type}
User clicks the spray bottle icon on the mood dashboard to open a scent selection modal displaying three fragrance options. Upon selection, the system updates the mood preset while keeping music and lighting unchanged.

\subsubsection{Adjust Scent Intensity}
On Manager or Scent device cards, a scent level slider allows users to adjust fragrance intensity from 1 to 10.

\subsubsection{Configure Spray Interval}
Users can set spray frequency through device settings with options for 5, 10, 15, 30, or 60-minute intervals.

\subsection{Music Control}
The music control section features play/pause, previous, and next track buttons below the album art. A progress bar displays current playback position with time indicators. Users can click the album art to open a song change modal and select from three song options.

\subsection{Lighting Control}
The lighting color is automatically generated by the AI mood system. Users can adjust brightness level through a slider control (0-100\%) on Light or Manager device cards. Manager devices also provide a color picker for manual color override.

\subsection{Mood Duration Bar}
The mood duration bar visualizes the 30-minute mood stream as an interactive timeline with 10 colored segments. Users can click any segment to jump to that specific mood state in the stream.

\subsection{Device Management}
The user can manage devices through the device grid section. This allows control of device power, settings, and deletion.

\subsubsection{Device Power Toggle}
Each device card displays a power toggle switch. Users can click the toggle to turn the device on or off. When powered off, control inputs are hidden and the device stops producing output.

\subsubsection{Delete Device}
Users can delete devices by clicking the delete button on expanded device cards. A confirmation modal appears to prevent accidental deletion.

\subsection{Save Favorite Mood}
The user can save the current mood as a favorite by clicking the star icon on the mood dashboard. Double-clicking the dashboard triggers a heart animation effect. Saved moods can be accessed later from the saved moods list.

\subsection{My Page}
Users can manage their personal settings and preferences by accessing the 'My Page' section through the bottom navigation bar. On the 'My Page' they can update their profile, view personal information, submit inquiries, and access account management options.

\subsubsection{Change Profile}
User can update their profile information, such as name, birthdate, gender, or profile picture by clicking the "Edit" button. After modifying fields, users click "Save" to persist changes.

\subsubsection{View Personal Information}
Users can check their personal information stored in the system, including email address, registration date, account provider (general/Google/Kakao/Naver), and survey completion status.

\subsubsection{View Q\&A}
The Q\&A tab displays frequently asked questions about the Mood Manager system, providing answers to common questions about mood recommendations and device management.

\subsubsection{Submit 1:1 Inquiry}
Users can submit support requests through the Inquiry tab. The form requires a title and content. After submission, the inquiry appears in the user's inquiry history with status indicators.

\subsubsection{Account Management}
The Privacy tab provides access to privacy policy and account deletion functionality. Users can permanently delete their account by clicking the "Delete Account" button, which requires password confirmation.

\subsection{Password Reset}
The user clicks the "Forgot password?" link on the login page to access password recovery. User enters their email to receive a 6-digit verification code. After verifying the code, user can set a new password and return to the login page.

\end{document}
