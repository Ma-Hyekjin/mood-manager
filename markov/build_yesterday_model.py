# build_yesterday_model_from_api_step2lite.py
# -*- coding: utf-8 -*-
"""
ì–´ì œ í•˜ë£¨(10ë¶„ ê°„ê²©) ì‹¤ì œ API raw ë°ì´í„°ë¥¼ ë°›ì•„ì„œ
â†’ step2-lite (ê²°ì¸¡ì¹˜ ë³´ê°„ + ffill/bfill, count=0, categorical ffill)
â†’ feature score 5ê°œ ê³„ì‚°
â†’ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° â†’ DTW í´ëŸ¬ìŠ¤í„°ë§ + ë§ˆë¥´ì½”í”„ + endpoint Î¼_k
â†’ debug_outputs/ ì´í•˜ì— ì¤‘ê°„ ì‚°ì¶œë¬¼ + yesterday_model_meta.json ì €ì¥.

âš ï¸ fetch_day_raw_from_api() ì•ˆë§Œ ì‹¤ì œ API í˜¸ì¶œ ë¡œì§ìœ¼ë¡œ ì±„ìš°ë©´ ë¨.
"""

from __future__ import annotations
from typing import List, Dict, Any, Tuple
from pathlib import Path
from datetime import datetime, timedelta
import json

import numpy as np
import pandas as pd
from tslearn.clustering import TimeSeriesKMeans


# ============================================================
# 0. ê³µí†µ ì„¤ì • + ë””ë ‰í† ë¦¬
# ============================================================

SLOT_MINUTES = 10             # 10ë¶„ ê°„ê²©
SLOTS_PER_DAY = 24 * 60 // SLOT_MINUTES  # 144
WINDOW_LENGTH = 24            # 24 * 10ë¶„ = 4ì‹œê°„
K_CLUSTERS = 5
RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)

BASE_DEBUG_DIR = Path("./debug_outputs")
RAW_DIR = BASE_DEBUG_DIR / "raw"
CLEAN_DIR = BASE_DEBUG_DIR / "clean"
FEAT_DIR = BASE_DEBUG_DIR / "features"
WIN_DIR = BASE_DEBUG_DIR / "windows"
CLUSTER_DIR = BASE_DEBUG_DIR / "clusters"
MODEL_DIR = BASE_DEBUG_DIR / "model"

for d in [BASE_DEBUG_DIR, RAW_DIR, CLEAN_DIR, FEAT_DIR, WIN_DIR, CLUSTER_DIR, MODEL_DIR]:
    d.mkdir(parents=True, exist_ok=True)


def clamp(v: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, v))


# ============================================================
# 1. step2-lite: ê²°ì¸¡ì¹˜ + ê°„ë‹¨ í´ë¦°
# ============================================================

def clean_raw_df_step2lite(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Step2-lite ë²„ì „
    - continuous: ì„ í˜• ë³´ê°„ + ffill + bfill
    - count: NaN -> 0
    - categorical: ffill + bfill
    - timestamp ì •ë ¬
    """
    df = raw_df.copy()

    # timestamp ì²˜ë¦¬ (APIì—ì„œ ë¬¸ìì—´ë¡œ ì˜¬ ê²ƒì„ ê°€ì •)
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.sort_values("timestamp").reset_index(drop=True)

    # ì»¬ëŸ¼ íƒ€ì… ë¶„ë¦¬ (ì‹¤ì œ API í•„ë“œëª…ì— ë§ê²Œ í•„ìš”í•˜ë©´ ìˆ˜ì •)
    continuous_cols_base = [
        "average_stress_index",
        "recent_stress_index",
        "latest_sleep_score",
        "latest_sleep_duration",
        "temperature",
        "humidity",
    ]
    count_cols_base = ["sigh", "laughter"]  # ex) sigh, laughter
    categorical_cols_base = ["rainType", "sky"]

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    continuous_cols = [c for c in continuous_cols_base if c in df.columns]
    count_cols = [c for c in count_cols_base if c in df.columns]
    categorical_cols = [c for c in categorical_cols_base if c in df.columns]

    # 1) continuous: interpolate + ffill + bfill
    if continuous_cols:
        df[continuous_cols] = (
            df[continuous_cols]
            .interpolate(method="linear", limit_direction="both")
            .ffill()
            .bfill()
        )

    # 2) count: NaN -> 0
    if count_cols:
        df[count_cols] = df[count_cols].fillna(0)

    # 3) categorical: ffill + bfill
    if categorical_cols:
        df[categorical_cols] = df[categorical_cols].ffill().bfill()

    return df


# ============================================================
# 2. feature score 5ê°œ ê³„ì‚°
# ============================================================

FEATURE_COLS = [
    "StressScore",
    "CalmScore",
    "FatigueScore",
    "VibrancyScore",
    "WeatherScore",
]


def compute_feature_row(raw_row: pd.Series) -> Dict[str, float]:
    """
    raw_row: Series
      - average_stress_index, recent_stress_index, latest_sleep_score, latest_sleep_duration,
        temperature, humidity, rainType, sky, laughter, sigh
    output:
      5ê°œ 0~1 score (Stress/Calm/Fatigue/Vibrancy/Weather)
    """
    # --- Stress / Calm ---
    avg_stress = raw_row["average_stress_index"] / 100.0
    recent_stress = raw_row["recent_stress_index"] / 100.0
    stress_raw = 0.4 * avg_stress + 0.6 * recent_stress
    StressScore = clamp(stress_raw, 0.0, 1.0)

    CalmScore = clamp(1.0 - StressScore * 0.9, 0.0, 1.0)

    # --- Fatigue ---
    sleep_score = raw_row["latest_sleep_score"] / 100.0
    sleep_dur_norm = raw_row["latest_sleep_duration"] / 600.0  # 0~600ë¶„
    sleep_fatigue = clamp(1.0 - 0.7 * sleep_score - 0.3 * sleep_dur_norm, 0.0, 1.0)

    sigh = raw_row["sigh"]
    sigh_norm = clamp(sigh / 10.0, 0.0, 1.0)
    FatigueScore = clamp(0.6 * sleep_fatigue + 0.4 * sigh_norm, 0.0, 1.0)

    # --- Vibrancy ---
    laughter = raw_row["laughter"]
    laugh_norm = clamp(laughter / 10.0, 0.0, 1.0)
    VibrancyScore = clamp(0.7 * laugh_norm + 0.3 * (1.0 - FatigueScore), 0.0, 1.0)

    # --- Weather ---
    temp = raw_row["temperature"]
    temp_discomfort = abs(temp - 22.0) / 20.0  # 22ë„ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ë¶ˆí¸
    humid = raw_row["humidity"]
    humid_discomfort = max(0.0, (humid - 60.0) / 40.0)

    rainType = raw_row["rainType"]
    sky = raw_row["sky"]
    rain_penalty = 0.0
    if rainType == 1:
        rain_penalty = 0.1
    elif rainType in (2, 3):
        rain_penalty = 0.2
    sky_penalty = 0.0
    if sky == 4:
        sky_penalty = 0.05

    discomfort = clamp(
        temp_discomfort + humid_discomfort + rain_penalty + sky_penalty,
        0.0,
        1.5,
    )
    WeatherScore = clamp(1.0 - discomfort, 0.0, 1.0)

    return {
        "StressScore": StressScore,
        "CalmScore": CalmScore,
        "FatigueScore": FatigueScore,
        "VibrancyScore": VibrancyScore,
        "WeatherScore": WeatherScore,
    }


def compute_feature_df(raw_df: pd.DataFrame) -> pd.DataFrame:
    feat_rows = []
    for _, row in raw_df.iterrows():
        scores = compute_feature_row(row)
        feat_rows.append(scores)
    df_feat = pd.DataFrame(feat_rows, index=raw_df["timestamp"])
    df_feat.index.name = "timestamp"
    return df_feat


# ============================================================
# 3. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
# ============================================================

def make_sliding_windows(arr: np.ndarray, L: int) -> Tuple[np.ndarray, List[int]]:
    """
    arr: (T, D)
    return:
      windows: (N, L, D)
      end_indices: ê° windowê°€ ëë‚˜ëŠ” index ë¦¬ìŠ¤íŠ¸
    """
    T, D = arr.shape
    if T < L:
        return np.empty((0, L, D)), []

    windows = []
    end_indices = []
    for end in range(L - 1, T):
        start = end - L + 1
        windows.append(arr[start:end+1])
        end_indices.append(end)

    return np.stack(windows, axis=0), end_indices


# ============================================================
# 4. DTW K-means + ë§ˆë¥´ì½”í”„ + endpoint Î¼_k
# ============================================================

def dtw_cluster(all_windows: np.ndarray, K: int = K_CLUSTERS) -> Tuple[np.ndarray, np.ndarray]:
    """
    all_windows: (N, L, D)
    return:
      labels: (N,)
      centroids: (K, L, D)
    """
    km = TimeSeriesKMeans(
        n_clusters=K,
        metric="dtw",
        metric_params={"sakoe_chiba_radius": 2},
        max_iter=5,
        n_init=1,
        random_state=RANDOM_SEED,
        n_jobs=-1,
        verbose=False,
    )
    labels = km.fit_predict(all_windows)
    centroids = km.cluster_centers_
    return labels, centroids


def compute_markov_transition(labels: np.ndarray, K: int, step: int = 1) -> np.ndarray:
    """
    labels ì‹œí€€ìŠ¤ë¡œë¶€í„° KxK ì „ì´í–‰ë ¬ ê³„ì‚°.
    step=1ì´ë©´ ë°”ë¡œ ë‹¤ìŒ, step=3ì´ë©´ 3 step í›„ ì „ì´ ì¶”ì •.
    """
    P = np.zeros((K, K), dtype=float)
    N = len(labels)
    for i in range(N - step):
        s = labels[i]
        t = labels[i + step]
        P[s, t] += 1.0

    for i in range(K):
        total = P[i].sum()
        if total > 0:
            P[i] /= total
        else:
            P[i, i] = 1.0
    return P


def compute_endpoint_means(windows: np.ndarray, labels: np.ndarray, K: int) -> np.ndarray:
    """
    ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ window ë§ˆì§€ë§‰ ì‹œì  featureì˜ í‰ê· (Î¼_k) ê³„ì‚°
    windows: (N, L, D)
    labels: (N,)
    return:
      endpoint_means: (K, D)
    """
    N, L, D = windows.shape
    endpoint_means = np.zeros((K, D), dtype=float)
    counts = np.zeros(K, dtype=int)

    for i in range(N):
        c = labels[i]
        x_end = windows[i, L - 1, :]
        endpoint_means[c] += x_end
        counts[c] += 1

    for k in range(K):
        if counts[k] > 0:
            endpoint_means[k] /= counts[k]
    return endpoint_means


# ============================================================
# 5. raw DataFrame â†’ yesterday_model_meta.json
# ============================================================

def build_yesterday_model_from_raw(
    raw_df: pd.DataFrame,
    save_prefix: str,
) -> Dict[str, Any]:
    """
    í•œ ìœ ì €, í•˜ë£¨ raw_df(10ë¶„ ê°„ê²©)ë¥¼ ë°›ì•„ì„œ
    Step2-lite í´ë¦° â†’ feature â†’ ìœˆë„ìš° â†’ í´ëŸ¬ìŠ¤í„°ë§ â†’ ë§ˆë¥´ì½”í”„ â†’ endpoint Î¼_k
    â†’ ëª¨ë¸ ë©”íƒ€/ì¤‘ê°„ ì‚°ì¶œë¬¼ ì €ì¥
    """

    # -------- (A) step2-lite clean --------
    clean_df = clean_raw_df_step2lite(raw_df)
    clean_path = CLEAN_DIR / f"{save_prefix}_clean.csv"
    clean_df.to_csv(clean_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ saved clean data to {clean_path}")

    # -------- (B) feature ê³„ì‚° + ì €ì¥ --------
    df_feat = compute_feature_df(clean_df)
    feat_path = FEAT_DIR / f"{save_prefix}_features.csv"
    df_feat.to_csv(feat_path, encoding="utf-8-sig")
    print(f"ğŸ’¾ saved features to {feat_path}")

    arr = df_feat.values.astype(float)    # (T, 5)

    # -------- (C) ìœˆë„ìš° ìƒì„± + ì €ì¥ --------
    windows, end_idx = make_sliding_windows(arr, L=WINDOW_LENGTH)
    if windows.shape[0] == 0:
        raise ValueError("ìœˆë„ìš° ìˆ˜ê°€ 0ì…ë‹ˆë‹¤. T < L ì¸ì§€ í™•ì¸ í•„ìš”.")

    win_path = WIN_DIR / f"{save_prefix}_windows_L{WINDOW_LENGTH}.npy"
    np.save(win_path, windows)
    print(f"ğŸ’¾ saved windows to {win_path} (shape={windows.shape})")

    end_timestamps = [df_feat.index[i] for i in end_idx]
    df_win_meta = pd.DataFrame({
        "window_id": list(range(len(end_idx))),
        "end_index": end_idx,
        "end_timestamp": end_timestamps,
    })
    win_meta_path = WIN_DIR / f"{save_prefix}_windows_meta_L{WINDOW_LENGTH}.csv"
    df_win_meta.to_csv(win_meta_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ saved window meta to {win_meta_path}")

    # -------- (D) í´ëŸ¬ìŠ¤í„°ë§ --------
    labels, centroids = dtw_cluster(windows, K=K_CLUSTERS)

    df_clusters = pd.DataFrame({
        "window_id": list(range(len(labels))),
        "end_timestamp": end_timestamps,
        "cluster": labels,
    })
    cluster_path = CLUSTER_DIR / f"{save_prefix}_cluster_labels.csv"
    df_clusters.to_csv(cluster_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ saved cluster labels to {cluster_path}")

    cent_path = MODEL_DIR / f"{save_prefix}_centroids_K{K_CLUSTERS}_L{WINDOW_LENGTH}.npy"
    np.save(cent_path, centroids)
    print(f"ğŸ’¾ saved centroids to {cent_path} (shape={centroids.shape})")

    # -------- (E) ë§ˆë¥´ì½”í”„ ì „ì´í–‰ë ¬ --------
    P1 = compute_markov_transition(labels, K_CLUSTERS, step=1)
    P3 = compute_markov_transition(labels, K_CLUSTERS, step=3)

    P1_path = MODEL_DIR / f"{save_prefix}_P1_step1.npy"
    P3_path = MODEL_DIR / f"{save_prefix}_P3_step3.npy"
    np.save(P1_path, P1)
    np.save(P3_path, P3)
    print(f"ğŸ’¾ saved P1 to {P1_path}, P3 to {P3_path}")

    # -------- (F) endpoint í‰ê·  Î¼_k --------
    endpoint_means = compute_endpoint_means(windows, labels, K_CLUSTERS)
    ep_path = MODEL_DIR / f"{save_prefix}_endpoint_means_K{K_CLUSTERS}.npy"
    np.save(ep_path, endpoint_means)
    print(f"ğŸ’¾ saved endpoint_means to {ep_path} (shape={endpoint_means.shape})")

    # -------- (G) í´ëŸ¬ìŠ¤í„° summary --------
    cluster_summaries = []
    for k in range(K_CLUSTERS):
        c = centroids[k]              # (L, 5)
        mean_vals = c.mean(axis=0)    # (5,)
        Stress, Calm, Fatigue, Vibrancy, Weather = mean_vals
        valence = (Calm + Vibrancy + Weather) - (Stress + Fatigue)
        arousal = (Stress + Vibrancy) / 2.0

        cluster_summaries.append({
            "cluster_id": k,
            "mean_scores": {
                "StressScore": float(Stress),
                "CalmScore": float(Calm),
                "FatigueScore": float(Fatigue),
                "VibrancyScore": float(Vibrancy),
                "WeatherScore": float(Weather),
            },
            "valence": float(valence),
            "arousal": float(arousal),
        })

    # -------- (H) ëª¨ë¸ ë©”íƒ€ json --------
    model_meta = {
        "freq_minutes": SLOT_MINUTES,
        "window_length": WINDOW_LENGTH,
        "K": K_CLUSTERS,
        "feature_cols": FEATURE_COLS,
        "centroids_npy": str(cent_path),
        "endpoint_means_npy": str(ep_path),
        "P1_npy": str(P1_path),
        "P3_npy": str(P3_path),
        "windows_npy": str(win_path),
        "windows_meta_csv": str(win_meta_path),
        "cluster_labels_csv": str(cluster_path),
        "features_csv": str(feat_path),
        "clean_csv": str(clean_path),
        "raw_note": "raw data ì €ì¥ì€ mainì—ì„œ ì²˜ë¦¬",
        "cluster_summaries": cluster_summaries,
    }

    model_json_path = MODEL_DIR / f"{save_prefix}_yesterday_model_meta.json"
    with model_json_path.open("w", encoding="utf-8") as f:
        json.dump(model_meta, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ saved model meta to {model_json_path}")

    runtime_model = {
        "freq_minutes": SLOT_MINUTES,
        "window_length": WINDOW_LENGTH,
        "K": K_CLUSTERS,
        "feature_cols": FEATURE_COLS,
        "centroids": centroids,
        "endpoint_means": endpoint_means,
        "P1": P1,
        "P3": P3,
        "cluster_summaries": cluster_summaries,
    }
    return runtime_model


# ============================================================
# 6. ì‹¤ì œ APIì—ì„œ í•˜ë£¨ raw ë°ì´í„° ë°›ì•„ì˜¤ê¸° (TODO)
# ============================================================

def fetch_day_raw_from_api(user_id: str, date: datetime) -> pd.DataFrame:
    """
    ì‹¤ì œ ì„œë¹„ìŠ¤ APIì—ì„œ ì–´ì œ í•˜ë£¨(10ë¶„ ê°„ê²©) raw ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜.

    âš ï¸ ì´ ë¶€ë¶„ì€ ì„œë¹„ìŠ¤ API ìŠ¤í™ì— ë§ê²Œ êµ¬í˜„í•´ì•¼ í•¨.
       ì•„ë˜ëŠ” "ì˜ˆì‹œ êµ¬ì¡°"ë§Œ ë³´ì—¬ì£¼ëŠ” í…œí”Œë¦¿ ì½”ë“œ.
    """
    # ì˜ˆì‹œ:
    # import requests
    # url = "https://your-service/api/mood/raw"
    # params = {
    #     "user_id": user_id,
    #     "date": date.strftime("%Y-%m-%d"),
    # }
    # r = requests.get(url, params=params)
    # r.raise_for_status()
    # data = r.json()
    # df = pd.DataFrame(data)
    # return df

    raise NotImplementedError("fetch_day_raw_from_api() ì•ˆì— ì‹¤ì œ API í˜¸ì¶œ ë¡œì§ì„ êµ¬í˜„í•˜ì„¸ìš”.")


# ============================================================
# 7. ë©”ì¸: ì–´ì œ ëª¨ë¸ ìƒì„±
# ============================================================

if __name__ == "__main__":
    user_id = "user_001"  # TODO: ì‹¤ì œ user_idë¡œ êµì²´

    today = datetime.today()
    yesterday = today - timedelta(days=1)
    date_str = yesterday.strftime("%Y%m%d")
    prefix = f"{user_id}_{date_str}"

    # 1) ì–´ì œ raw ë°ì´í„° APIì—ì„œ ê°€ì ¸ì˜¤ê¸°
    raw_df = fetch_day_raw_from_api(user_id, yesterday)

    # raw csv ì €ì¥
    raw_path = RAW_DIR / f"{prefix}_raw.csv"
    raw_df.to_csv(raw_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ saved raw data to {raw_path}")

    if raw_df.shape[0] != SLOTS_PER_DAY:
        print(f"[WARN] expected {SLOTS_PER_DAY} rows, got {raw_df.shape[0]} rows")

    # 2) ëª¨ë¸ ìƒì„± + ì €ì¥
    yesterday_model = build_yesterday_model_from_raw(raw_df, save_prefix=prefix)

    print("\n=== Yesterday model built ===")
    print("K:", yesterday_model["K"])
    for s in yesterday_model["cluster_summaries"]:
        print(s)
